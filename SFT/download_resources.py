import os
# å¼ºåˆ¶ä½¿ç”¨ HF é•œåƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer
from datasets import load_dataset

# å®šä¹‰ä¿å­˜è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # d:\BiSHE
LOCAL_DIR = os.path.join(ROOT_DIR, "local_resources")

if not os.path.exists(LOCAL_DIR):
    os.makedirs(LOCAL_DIR)

print(f"ğŸ“‚ èµ„æºå°†ä¿å­˜åˆ°: {LOCAL_DIR}")

# 1. ä¸‹è½½ Alpaca æ•°æ®
print("\n[1/3] æ­£åœ¨ä¸‹è½½ Alpaca æ•°æ®é›† (çº¦ 25MB)...")
try:
    ds = load_dataset("tatsu-lab/alpaca")
    save_path = os.path.join(LOCAL_DIR, "alpaca_data")
    ds.save_to_disk(save_path)
    print(f"âœ… æ•°æ®é›†å·²ä¿å­˜è‡³: {save_path}")
except Exception as e:
    print(f"âŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")

# 2. ä¸‹è½½ Embedding æ¨¡å‹
print("\n[2/3] æ­£åœ¨ä¸‹è½½ Embedding æ¨¡å‹ (all-MiniLM-L6-v2, çº¦ 80MB)...")
try:
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    model = SentenceTransformer(model_name)
    save_path = os.path.join(LOCAL_DIR, "embed_model")
    model.save(save_path)
    print(f"âœ… Embedding æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
except Exception as e:
    print(f"âŒ Embedding æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")

# 3. ä¸‹è½½ LLM æ¨¡å‹
print("\n[3/3] æ­£åœ¨ä¸‹è½½ Qwen1.5-0.5B æ¨¡å‹ (çº¦ 1.2GB)...")
print("â³ è¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
try:
    model_id = "Qwen/Qwen1.5-0.5B"
    save_path = os.path.join(LOCAL_DIR, "qwen_model")
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.save_pretrained(save_path)
    
    model = AutoModelForCausalLM.from_pretrained(model_id)
    model.save_pretrained(save_path)
    print(f"âœ… Qwen æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
except Exception as e:
    print(f"âŒ Qwen æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")

print("\nğŸ‰ æ‰€æœ‰ä¸‹è½½ä»»åŠ¡ç»“æŸï¼")
