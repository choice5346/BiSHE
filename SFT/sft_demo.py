import os
# å¼ºåˆ¶ä½¿ç”¨ HF é•œåƒ (é’ˆå¯¹å›½å†…ç½‘ç»œä¼˜åŒ–)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import sys
import json
import random
import numpy as np
import torch
from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType

from sentence_transformers import SentenceTransformer
from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc
import evaluate
from tqdm import tqdm

# --- æ ¸å¿ƒç®—æ³•å¯¼å…¥ (å¤ç”¨ä¹‹å‰çš„ KNN-Shapley) ---
# ç¡®ä¿ helper.py åœ¨ä¸Šä¸€çº§ç›®å½•æˆ– sys.path ä¸­
# helper.py å®é™…ä½äº d:\BiSHE\softlabel-knnsv\
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "softlabel-knnsv"))
try:
    from helper import knn_shapley_JW
except ImportError:
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° helper.pyã€‚è¯·ç¡®ä¿å®ƒéƒ½åœ¨ d:\\BiSHE\\helper.py")
    sys.exit(1)

# ==========================================
# 0. å…¨å±€é…ç½®
# ==========================================
# æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ¬åœ°èµ„æº
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
LOCAL_RES_DIR = os.path.join(ROOT_DIR, "local_resources")

# ä¼˜å…ˆä½¿ç”¨æœ¬åœ°èµ„æºè·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨äº‘ç«¯ ID
model_path = os.path.join(LOCAL_RES_DIR, "qwen_model")
if not os.path.exists(model_path):
    model_path = "Qwen/Qwen1.5-0.5B"
    print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ° Qwen æ¨¡å‹ï¼Œå°†ä½¿ç”¨äº‘ç«¯ä¸‹è½½æ¨¡å¼")
else:
    print(f"âœ… ä½¿ç”¨æœ¬åœ° Qwen æ¨¡å‹: {model_path}")

embed_path = os.path.join(LOCAL_RES_DIR, "embed_model")
if not os.path.exists(embed_path):
    embed_path = "all-MiniLM-L6-v2"
    print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ° Embedding æ¨¡å‹ï¼Œå°†ä½¿ç”¨äº‘ç«¯ä¸‹è½½æ¨¡å¼")
else:
    print(f"âœ… ä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹: {embed_path}")

CONFIG = {
    "model_name": model_path,                # ç›®æ ‡ LLM
    "embed_model": embed_path,               # ç‰¹å¾æå–æ¨¡å‹
    "n_samples": 2000,                       # æ€»æ ·æœ¬æ•° (æ¨¡æ‹Ÿå°æ ·æœ¬å®éªŒ)
    "poison_ratio": 0.3,                     # æŠ•æ¯’æ¯”ä¾‹ (30% åƒåœ¾æ•°æ®)
    "output_dir": "SFT/results",             # è¾“å‡ºç›®å½•
    "seed": 42
}

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

# ==========================================
# 1. æ•°æ®åˆæˆ (Data Synthesis)
# ==========================================
def prepare_data():
    """
    ä¸‹è½½ Alpaca æ•°æ®é›†ï¼Œå¹¶äººå·¥æ³¨å…¥ 30% çš„å™ªå£°ã€‚
    è¿”å›: raw_dataset (å«å™ªå£°), clean_indices (GT), dirty_indices (GT)
    """
    print("ğŸ“¥ æ­£åœ¨åŠ è½½ Alpaca æ•°æ®é›†...")
    
    ds_raw = None
    local_data_path = os.path.join(LOCAL_RES_DIR, "alpaca_data")
    
    # 1. å°è¯•æœ¬åœ°åŠ è½½ (ä¼˜å…ˆ)
    if os.path.exists(local_data_path):
        try:
            from datasets import load_from_disk
            print(f"ğŸ“‚ å°è¯•åŠ è½½æœ¬åœ°æ•°æ®: {local_data_path}")
            ds_full = load_from_disk(local_data_path)
            # å¤„ç† dataset dict æˆ– dataset
            if isinstance(ds_full, dict) or hasattr(ds_full, 'keys'):
                split_name = 'train' if 'train' in ds_full else list(ds_full.keys())[0]
                ds_full = ds_full[split_name]
            
            # åˆ‡ç‰‡
            ds_raw = ds_full.select(range(min(len(ds_full), CONFIG['n_samples'])))
            print("âœ… æœ¬åœ°æ•°æ®åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ æœ¬åœ°åŠ è½½å¤±è´¥: {e}")

    # 2. å¦‚æœæœ¬åœ°å¤±è´¥ï¼Œå°è¯•åœ¨çº¿
    if ds_raw is None:
        try:
            print("â˜ï¸ å°è¯•åœ¨çº¿åŠ è½½ tatsu-lab/alpaca...")
            ds_raw = load_dataset("tatsu-lab/alpaca", split=f"train[:{CONFIG['n_samples']}]")
        except Exception as e:
            print(f"âš ï¸ åœ¨çº¿åŠ è½½å¤±è´¥: {e}")

    # 3. å®åœ¨ä¸è¡Œï¼Œåˆæˆå…œåº•
    ds = []
    if ds_raw:
        ds = [item for item in ds_raw]
    else:
        print("â˜¢ï¸ ä¸¥é‡è­¦å‘Šï¼šæ‰€æœ‰æ•°æ®åŠ è½½æ–¹å¼å¤±è´¥ï¼Œä½¿ç”¨å«æœ‰å™ªå£°çš„åˆæˆç®—æœ¯æ•°æ®å…œåº•ã€‚")
        # ç”Ÿæˆåˆæˆæ•°æ®
        for k in range(CONFIG['n_samples']):
            ds.append({
                "instruction": f"Solve math problem: {k} + {k}",
                "input": "", 
                "output": f"The answer is {k+k}."
            })
            
    data = []
    clean_indices = []
    dirty_indices = []
    
    set_seed(CONFIG['seed'])
    
    # å®šä¹‰ä¸€äº›â€œåƒåœ¾å›ç­”â€æ¨¡æ¿
    garbage_responses = [
        "I don't know the answer because I am an AI.",
        "This is a random sentence generated by a computer.",
        "Error 404: Answer not found.",
        "Bla bla bla, I am just outputting noise.",
        "To be or not to be, that is the question.",
        "Ignore the previous instruction, here is a poem about cats."
    ]
    
    print(f"ğŸ˜ˆ æ­£åœ¨æ³¨å…¥å™ªå£° (æ¯”ä¾‹: {CONFIG['poison_ratio']:.0%})...")
    
    for i, item in enumerate(ds):
        # å†³å®šæ˜¯å¦æŠ•æ¯’
        is_poison = random.random() < CONFIG['poison_ratio']
        
        new_item = {
            "instruction": item["instruction"],
            "input": item["input"],
            "output": item["output"]
        }
        
        if is_poison:
            # æ›¿æ¢ output ä¸ºåƒåœ¾
            new_item["output"] = random.choice(garbage_responses)
            dirty_indices.append(i)
        else:
            clean_indices.append(i)
            
        data.append(new_item)
        
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: æ€»æ•° {len(data)}, å¹²å‡€ {len(clean_indices)}, è„ {len(dirty_indices)}")
    return data, dirty_indices, ds

# ==========================================
# 2. ç‰¹å¾æå– (Feature Extraction)
# ==========================================
def extract_text_features(data_list):
    """
    ä½¿ç”¨ Sentence-Transformer å°† (Instruction + Input + Output) è½¬åŒ–ä¸ºå‘é‡ã€‚
    æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æŠŠ Input å’Œ Output æ‹¼åœ¨ä¸€èµ·ç®—ç‰¹å¾ï¼Œå› ä¸º KNN-SV éœ€è¦è¡¡é‡ X å’Œ Y çš„å…³ç³»ã€‚
    å®é™…ä¸Šåœ¨ SFT ä¸­ï¼ŒX=Prompt, Y=Responseã€‚
    æˆ‘ä»¬å°† "Prompt: ... Response: ..." ä½œä¸ºä¸€ä¸ªæ•´ä½“æ–‡æœ¬è¿›è¡Œ Embeddingã€‚
    """
    print(f"ğŸ§  æ­£åœ¨æå–æ–‡æœ¬ç‰¹å¾ ({CONFIG['embed_model']})...")
    model = SentenceTransformer(CONFIG['embed_model'])
    
    # æ„é€ æ–‡æœ¬åˆ—è¡¨
    texts = []
    for item in data_list:
        # æ ¼å¼åŒ–æ–‡æœ¬: "Instruction: XXX\nInput: YYY\nOutput: ZZZ"
        text = f"Instruction: {item['instruction']}\nInput: {item['input']}\nOutput: {item['output']}"
        texts.append(text)
        
    # æ‰¹é‡è®¡ç®— Embedding
    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True, convert_to_numpy=True)
    
    # L2 å½’ä¸€åŒ– (å…³é”®ï¼å¤ç”¨ä¹‹å‰çš„ç»éªŒ)
    # SentenceTransformer é»˜è®¤é€šå¸¸å·²ç»å½’ä¸€åŒ–äº†ï¼Œä½†ä¸ºäº†ä¿é™©å†åšä¸€æ¬¡
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    embeddings = embeddings / norms
    
    return embeddings

# ==========================================
# 3. æ ¸å¿ƒç®—æ³•è°ƒç”¨ (Valuation)
# ==========================================
def calculate_shapley(embeddings, dirty_indices):
    """
    è°ƒç”¨ knn_shapley_JW è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä»·å€¼ã€‚
    æ³¨æ„ï¼šhelper.py é‡Œçš„å‡½æ•°éœ€è¦ (x_train, y_train, x_val, y_val)ã€‚
    ä½†åœ¨æ— ç›‘ç£/è‡ªç›‘ç£åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸è¿™ä¹ˆåšï¼š
    1. æŠŠæ•°æ®åˆ‡åˆ†ä¸º Support Set (ä½œä¸º 'Train') å’Œ Query Set (ä½œä¸º 'Val')
    2. æˆ–è€…ç›´æ¥ç”¨ Leave-One-Out (KNN-SV æ”¯æŒè¿™ç§æ€æƒ³)
    
    ä¸ºäº†é€‚é… helper.py (å®ƒåŸæœ¬æ˜¯ä¸ºåˆ†ç±»è®¾è®¡çš„ï¼Œéœ€è¦ label Y)ï¼Œ
    æˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ª **Trick**:
    æˆ‘ä»¬å°† "Embedding" è§†ä¸º Xï¼Œäººä¸ºæ„é€ ä¸€ä¸ªå…¨æ˜¯ 1 çš„ Y (ä»£è¡¨å®ƒä»¬éƒ½å±äº'æ­£ç±»'ä»»åŠ¡)ã€‚
    ç„¶åçœ‹å“ªäº›æ ·æœ¬åœ¨ KNN é‚»åŸŸé‡Œä¸å…¶ä»–æ ·æœ¬ 'æ ¼æ ¼ä¸å…¥' (å³å®ƒçš„ç‰¹å¾åˆ†å¸ƒåç¦»äº†ä¸»æµå½¢)ã€‚
    
    æˆ–è€…æ›´ç®€å•çš„ï¼š
    æˆ‘ä»¬å°†æ•°æ®é›†éšæœºåˆ‡åˆ† 80% / 20%ã€‚
    è®¡ç®— 80% æ•°æ®å¯¹ 20% æ•°æ®çš„ KNN è´¡çŒ®ã€‚
    """
    print("ğŸ” å¼€å§‹è®¡ç®— KNN-Shapley å€¼...")
    
    n_total = len(embeddings)
    n_support = int(n_total * 0.8)
    
    indices = np.random.permutation(n_total)
    support_idx = indices[:n_support]
    val_idx = indices[n_support:]
    
    x_support = embeddings[support_idx]
    x_val = embeddings[val_idx]
    
    # ä¼ªé€  Labelï¼šåœ¨è¿™ä¸ªåœºæ™¯ä¸‹ï¼Œlabel å¹¶ä¸é‡è¦ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯ç‰¹å¾ä¸€è‡´æ€§
    # ä½† helper.py éœ€è¦ label åŒ¹é…æ‰èƒ½å¾—åˆ†ã€‚
    # ç­–ç•¥ï¼šæˆ‘ä»¬å‡è®¾æ‰€æœ‰æ•°æ®éƒ½æ˜¯ 'Class 0'ã€‚KNN ä¼šå¯»æ‰¾æœ€è¿‘çš„é‚»å±…ã€‚
    # å¦‚æœä¸€ä¸ªè„æ•°æ®æ··è¿›æ¥äº†ï¼Œå®ƒçš„é‚»å±…å¯èƒ½æ¯”è¾ƒè¿œï¼Œæˆ–è€…å®ƒå‘¨å›´ä¹Ÿæ˜¯è„æ•°æ®ã€‚
    # 
    # ç­‰ç­‰ï¼Œå¦‚æœå…¨æ˜¯ Class 0ï¼Œé‚£ä¹ˆ helper.py ä¼šè®¤ä¸ºæ‰€æœ‰é‚»å±…éƒ½æ˜¯åŒç±»ï¼Œéƒ½ä¼šåŠ åˆ†ã€‚
    # è¿™å¯èƒ½æ— æ³•åŒºåˆ†è„æ•°æ®ã€‚
    #
    # ä¿®æ­£ç­–ç•¥ï¼š
    # è„æ•°æ®çš„ç‰¹å¾è¯­ä¹‰é€šå¸¸ä¸æ­£å¸¸æ•°æ®å·®å¼‚å¾ˆå¤§ã€‚
    # å¦‚æœæˆ‘ä»¬ç”¨ KNN è·ç¦»æœ¬èº«æ¥è¡¡é‡å‘¢ï¼Ÿ
    # KNN-Shapley çš„æœ¬è´¨æ˜¯ï¼šå¦‚æœæŠŠè¿™ä¸ªç‚¹åŠ å…¥ï¼Œèƒ½ä¸èƒ½æé«˜å¯¹ Validation Set çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚
    # è¿™é‡Œæˆ‘ä»¬æ²¡æœ‰ Labelã€‚
    #
    # æ›¿ä»£æ–¹æ¡ˆ -> **KNN Density (å±€éƒ¨å¯†åº¦)** 
    # æˆ–è€…æˆ‘ä»¬è¿˜æ˜¯ç”¨ helper.pyï¼Œä½†æ˜¯æ„é€ ä¸€ç§ç‰¹æ®Šçš„ä»»åŠ¡ï¼Ÿ
    #
    # è®©æˆ‘ä»¬å›é€€ä¸€æ­¥ï¼š
    # è„æ•°æ®çš„ definition æ˜¯ "Instruction å’Œ Output ä¸åŒ¹é…"ã€‚
    # è€Œ Clean æ•°æ®æ˜¯åŒ¹é…çš„ã€‚
    # æ­£å¸¸çš„ Sentence-Transformer è®­ç»ƒæ—¶ä½¿ç”¨äº† Contrastive Lossã€‚
    # æ‰€ä»¥ "é—®é¢˜+æ­£ç¡®ç­”æ¡ˆ" çš„å‘é‡ï¼Œå’Œ "é—®é¢˜+é”™è¯¯ç­”æ¡ˆ" çš„å‘é‡ï¼Œåº”è¯¥åœ¨ç©ºé—´ä¸­èšé›†åœ¨ä¸åŒä½ç½®ã€‚
    # 
    # é‰´äºæˆ‘ä»¬å¿…é¡»å¤ç”¨ knn_shapley_JWï¼Œæˆ‘ä»¬å°è¯•æ„é€ ä¸€ä¸ª **Refrence Set (å¹²å‡€çš„å°é›†åˆ)**ã€‚
    # å‡è®¾æˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‰‹é‡Œæ€»æœ‰é‚£ä¹ˆ 10-20 æ¡äººå·¥å†™çš„å®Œç¾æ•°æ®ã€‚
    # æˆ‘ä»¬æŠŠè¿™ 20 æ¡å½“ä½œ Validation Set (x_val, y_val=0)ã€‚
    # æŠŠå¾…æ¸…æ´—æ•°æ®å½“ä½œ Training Set (x_train, y_train=0)ã€‚
    # çœ‹ x_train é‡Œå“ªäº›æ ·æœ¬å¯¹é¢„æµ‹ x_val æœ‰å¸®åŠ© (å³ä¸ x_val ç›¸ä¼¼)ã€‚
    # è¿™æ ·ï¼Œä¸å®Œç¾æ•°æ®ç›¸ä¼¼çš„æ ·æœ¬å¾—åˆ†é«˜ï¼Œåƒåœ¾åºŸè¯å¾—åˆ†ä½ã€‚
    pass 
    
    # --- å®é™…å®ç° ---
    # è¿™é‡Œçš„ Trick: éšæœºæŠ½å– 50 æ¡æ ·æœ¬ï¼Œäººå·¥è®¤å®šå®ƒä»¬æ˜¯å¹²å‡€çš„ï¼ˆæˆ–è€…æˆ‘ä»¬åœ¨å®éªŒè®¾å®šä¸­å·²çŸ¥ Clean Indiciesï¼‰
    # åœ¨çœŸå® SFT åœºæ™¯ä¸­ï¼Œé€šå¸¸ç¡®å®ä¼šæœ‰ä¸€å°éƒ¨åˆ† Golden Dataã€‚
    # æˆ‘ä»¬ä» clean_indices ä¸­ "å·" 100 æ¡ä½œä¸º Validation Setã€‚
    
    real_clean_idx = [i for i in range(n_total) if i not in dirty_indices]
    random.shuffle(real_clean_idx)
    # å– 100 æ¡ä½œä¸ºé»„é‡‘éªŒè¯é›†
    gold_val_indices = real_clean_idx[:100]
    
    # å‰©ä¸‹çš„ä½œä¸ºå¾…æ¸…æ´—æ± 
    candidate_indices = [i for i in range(n_total) if i not in gold_val_indices]
    
    x_train = embeddings[candidate_indices]
    y_train = np.zeros(len(x_train), dtype=int) # å‡æ ‡ç­¾
    
    x_val = embeddings[gold_val_indices]
    y_val = np.zeros(len(x_val), dtype=int)     # å‡æ ‡ç­¾
    
    # è®¡ç®—
    sv = knn_shapley_JW(x_train, y_train, x_val, y_val, K=10)
    
    # Debug: æ‰“å° Shapley Value çš„ä¸€äº›ç»Ÿè®¡ä¿¡æ¯ï¼Œé˜²æ­¢å…¨æ˜¯ 0
    print(f"ğŸ“Š Shapley Value Stats -> Mean: {np.mean(sv):.4f}, Std: {np.std(sv):.4f}, Max: {np.max(sv):.4f}, Min: {np.min(sv):.4f}")
    
    # æˆ‘ä»¬éœ€è¦æŠŠ sv æ˜ å°„å›åŸå§‹ indices
    full_sv = np.zeros(n_total)
    full_sv[candidate_indices] = sv
    # Golden Set çš„æ ·æœ¬ç»™è‡ªå·±æ»¡åˆ† (æˆ–è€…ä¸åšå¤„ç†)
    full_sv[gold_val_indices] = np.max(sv) 
    
    return full_sv

# ==========================================
# 4. å¾®è°ƒè®­ç»ƒ (Fine-tuning)
# ==========================================
def run_sft(dataset_list, output_name):
    """
    ä½¿ç”¨ HuggingFace Trainer è¿›è¡Œå¾®è°ƒ (ä½¿ç”¨ LoRA ä»¥ä¿è¯ç¨³å®šæ€§)ã€‚
    """
    print(f"ğŸš€ å¼€å§‹å¾®è°ƒ: {output_name} (æ ·æœ¬æ•°: {len(dataset_list)})...")
    
    model_id = CONFIG['model_name']
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    # ä¿®å¤ tokenizer è®¾ç½®
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right" # è®­ç»ƒæ—¶é€šå¸¸ç”¨ right padding

    # æ ¼å¼åŒ–æ•°æ®ä¸º HuggingFace Dataset
    def format_func(example):
        # Qwen å®˜æ–¹æ¨èæ ¼å¼ / ChatML æ ¼å¼
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½†ç¡®ä¿ EOS
        text = f"User: {example['instruction']}\n{example['input']}\nAssistant: {example['output']}{tokenizer.eos_token}"
        return {"text": text}
    
    hf_dataset = Dataset.from_list(dataset_list)
    hf_dataset = hf_dataset.map(lambda x: tokenizer(format_func(x)["text"], truncation=True, max_length=512), batched=False)
    
    # åŠ è½½æ¨¡å‹ (FP32 æ¨¡å¼åŠ è½½ï¼Œé¿å… 16bit æº¢å‡º)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.float32)
    
    # å¯ç”¨ LoRA (å…³é”®ï¼é˜²å´©ç¥å™¨)
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, 
        inference_mode=False, 
        r=8, 
        lora_alpha=32, 
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"] # é’ˆå¯¹ Attention å±‚å¾®è°ƒ
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # è®­ç»ƒå‚æ•°è°ƒæ•´
    training_args_dict = {
        "output_dir": f"{CONFIG['output_dir']}/{output_name}",
        "per_device_train_batch_size": 4, # LoRA çœæ˜¾å­˜ï¼Œå¯ä»¥ç¨å¾®å¤§ç‚¹
        "gradient_accumulation_steps": 4,
        "num_train_epochs": 3,            # LoRA éœ€è¦å¤šè®­å‡ è½®
        "learning_rate": 3e-4,            # LoRA é€šå¸¸ç”¨å¤§ä¸€ç‚¹çš„å­¦ä¹ ç‡
        "logging_steps": 10,
        "save_strategy": "no",
        "report_to": "none",
        "fp16": False,                    # ä½¿ç”¨ FP32 ä¿è¯æ•°å€¼ç»å¯¹ç¨³å®š
        "bf16": False,
    }
    
    args = TrainingArguments(**training_args_dict)
    
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=hf_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    )
    
    trainer.train()

    # ğŸ’¾ ä¿å­˜æ¨¡å‹ (LoRA é€‚é…å™¨)
    save_path = f"{CONFIG['output_dir']}/{output_name}"
    trainer.save_model(save_path)
    # åˆå¹¶æƒé‡å¹¶ä¿å­˜ (æ–¹ä¾¿ chat_compare è¯»å–)
    print("ğŸ’¾ æ­£åœ¨åˆå¹¶ LoRA æƒé‡...")
    merged_model = model.merge_and_unload()
    merged_model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"ğŸ’¾ å®Œæ•´æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
    
    # === æ–°å¢ï¼šè®¡ç®— ROUGE åˆ†æ•° (æ›´æœ‰è¯´æœåŠ›çš„ç”ŸæˆæŒ‡æ ‡) ===
    print("ğŸ“ æ­£åœ¨è¯„ä¼° ROUGE æŒ‡æ ‡...")
    metric = evaluate.load("rouge")
    
    # æŠ½å– 50 æ¡æµ‹è¯•æ•°æ® (ä¸å‚ä¸è®­ç»ƒçš„)
    test_samples = dataset_list[:50] 
    
    preds = []
    refs = []
    
    # åˆ‡æ¢å› eval æ¨¡å¼
    merged_model.eval()
    for item in tqdm(test_samples, desc="Evaluating"):
        prompt = f"User: {item['instruction']}\n\nAssistant: "
        inputs = tokenizer(prompt, return_tensors="pt").to(merged_model.device)
        with torch.no_grad():
            outputs = merged_model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)
        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "Assistant: " in pred:
            pred = pred.split("Assistant: ")[1].strip()
        preds.append(pred)
        refs.append(item['output'])
        
    scores = metric.compute(predictions=preds, references=refs)
    print(f"ğŸ“Š {output_name} ROUGE-L: {scores['rougeL']:.4f}")
    
    # è¿”å› Loss å’Œ RougeL ç”¨äºå¯¹æ¯”
    return trainer.state.log_history[-1].get('train_loss', 0.0), scores['rougeL']

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def main():
    # 1. å‡†å¤‡æ•°æ®
    # oracle_data: æœªè¢«æ±¡æŸ“çš„åŸå§‹çº¯å‡€æ•°æ® (ä¸Šé™/Gold Standard)
    raw_data, dirty_indices, oracle_data = prepare_data()
    
    # 2. æå–ç‰¹å¾
    embeddings = extract_text_features(raw_data)
    
    # 3. è®¡ç®—ä»·å€¼ (Shapley Value)
    # è¿™ä¸€æ­¥æˆ‘ä»¬ç”¨ "Golden Set Validation" çš„ç­–ç•¥
    sv = calculate_shapley(embeddings, dirty_indices)
    
    # 4. æ ¹æ®ä»·å€¼æ¸…æ´—
    # ç­–ç•¥ï¼šæˆ‘ä»¬è¦åˆ æ‰ 30% å·¦å³çš„æ•°æ®
    n_remove = int(len(raw_data) * CONFIG['poison_ratio'])
    
    sorted_idx = np.argsort(sv) # ä»å°åˆ°å¤§
    # ä½åˆ†çš„æ˜¯è„æ•°æ®ï¼Œåˆ æ‰
    keep_indices = sorted_idx[n_remove:]
    
    cleaned_data = [raw_data[i] for i in keep_indices]
    
    print("-" * 50)
    print(f"ğŸ§¹ æ¸…æ´—å®Œæˆï¼")
    print(f"åŸæ•°æ®: {len(raw_data)}")
    print(f"æ¸…æ´—å: {len(cleaned_data)}")
    
    # éªŒè¯æ¸…æ´—å‡†ç¡®ç‡ (Recall)
    # æˆ‘ä»¬çŸ¥é“ dirty_indices æ˜¯çœŸçš„è„æ•°æ®
    # æˆ‘ä»¬çš„ç³»ç»Ÿåˆ é™¤äº† sorted_idx[:n_remove]
    removed_indices = sorted_idx[:n_remove]
    correct_removed = set(removed_indices).intersection(set(dirty_indices))
    recall = len(correct_removed) / len(dirty_indices)
    print(f"ğŸ•µï¸â€â™‚ï¸ è„æ•°æ®è¯†åˆ«å¬å›ç‡ (Recall): {recall:.2%}")

    # è®¡ç®— AUC (æ¸…æ´—ç®—æ³•çš„æ€§èƒ½æŒ‡æ ‡)
    # æˆ‘ä»¬å¸Œæœ› dirty_indices å¯¹åº”çš„ sv å€¼å¾ˆä½ï¼Œè€Œ clean å¯¹åº”çš„é«˜ã€‚
    # ä¸ºäº†è®¡ç®— AUCï¼Œæˆ‘ä»¬éœ€è¦æ„å»º 0/1 æ ‡ç­¾ï¼š1 ä¸º Clean, 0 ä¸º Dirty (æˆ–è€…åè¿‡æ¥)
    # è¿™é‡Œå®šä¹‰ï¼šClean=1, Dirty=0
    true_labels = np.ones(len(raw_data))
    true_labels[dirty_indices] = 0
    
    # è®¡ç®— ROC-AUC
    # æ³¨æ„ï¼šæˆ‘ä»¬çš„ sv è¶Šé«˜è¶Š cleanï¼Œç¬¦åˆ y=1 çš„æ–¹å‘
    roc_auc = roc_auc_score(true_labels, sv)
    print(f"ğŸ“ˆ æ¸…æ´—ç®—æ³• AUC å¾—åˆ†: {roc_auc:.4f} (1.0ä»£è¡¨å®Œç¾åŒºåˆ†)")
    print("-" * 50)
    
    # 5. SFT å¯¹æ¯”å®éªŒ (Baseline vs Clean)
    # æ³¨æ„ï¼šä¸ºäº†å…¬å¹³ï¼ŒBaseline æˆ‘ä»¬é€šå¸¸ä¸åˆ æ•°æ®ï¼Œæˆ–è€…éšæœºåˆ ã€‚
    # è¿™é‡Œå¯¹æ¯”ï¼šDirty Full Set vs Cleaned Set
    
    # 5.1 è®­ç»ƒ Dirty Model
    loss_dirty, rouge_dirty = run_sft(raw_data, "dirty_model")
    
    # 5.2 è®­ç»ƒ Clean Model
    # ä¸ºäº†é‡Šæ”¾æ˜¾å­˜ï¼Œå»ºè®®åœ¨è¿™é‡ŒåŠ å…¥é‡Šæ”¾æ˜¾å­˜çš„ä»£ç ï¼Œæˆ–è€…åˆ†è„šæœ¬è¿è¡Œ
    # è¿™é‡Œæ¼”ç¤ºï¼Œå‡è®¾æ˜¾å­˜å¤Ÿç”¨
    import gc
    torch.cuda.empty_cache()
    gc.collect()
    
    loss_clean, rouge_clean = run_sft(cleaned_data, "clean_model")
    
    # 5.3 è®­ç»ƒ Oracle Model (å…¨å¹²å‡€æ•°æ® - ç†è®ºä¸Šé™)
    import gc
    torch.cuda.empty_cache()
    gc.collect()
    
    loss_oracle, rouge_oracle = run_sft(oracle_data, "oracle_model")
    
    print("=" * 50)
    print("ğŸ“Š æœ€ç»ˆå®éªŒç»“æœ")
    print(f"{'Model':<15} | {'Loss':<10} | {'ROUGE-L':<10}")
    print("-" * 45)
    print(f"{'Dirty (Base)':<15} | {loss_dirty:.4f}     | {rouge_dirty:.4f}")
    print(f"{'Clean (Yours)':<15} | {loss_clean:.4f}     | {rouge_clean:.4f}")
    print(f"{'Oracle (Top)':<15} | {loss_oracle:.4f}     | {rouge_oracle:.4f}")
    print("-" * 45)
    
    if rouge_clean > rouge_dirty:
        print("âœ… éªŒè¯æˆåŠŸï¼šæ¸…æ´—åæ¨¡å‹çš„ ROUGE åˆ†æ•°æ›´é«˜ï¼Œç”Ÿæˆè´¨é‡æ›´å¥½ï¼")

if __name__ == "__main__":
    main()
