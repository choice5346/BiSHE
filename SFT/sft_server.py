import os
import sys
import json
import random
import numpy as np
import torch
import shutil
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import torch.nn.functional as F
from tqdm import tqdm

# ==========================================
# 0. ç¯å¢ƒä¸è·¯å¾„é…ç½®
# ==========================================

# è®¾ç½® HF é•œåƒ (é’ˆå¯¹å›½å†…ç½‘ç»œ)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å®šä¹‰æœ¬åœ°èµ„æºä¿å­˜è·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# èµ„æºä¿å­˜åœ¨å½“å‰è„šæœ¬åŒçº§ç›®å½•ä¸‹çš„ server_resources
RESOURCES_DIR = os.path.join(CURRENT_DIR, "server_resources")
# è¿™é‡Œç›´æ¥è¯»å– alpaca_data.json æ–‡ä»¶
DATASET_PATH = os.path.join(RESOURCES_DIR, "alpaca_data.json")
MODEL_PATH = os.path.join(RESOURCES_DIR, "qwen_model")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(RESOURCES_DIR, exist_ok=True)

CONFIG = {
    # è‡ªåŠ¨è¯†åˆ«ï¼šå¦‚æœæœ¬åœ° MODEL_PATH é‡Œæœ‰æ–‡ä»¶ï¼Œå°±ç”¨æœ¬åœ°è·¯å¾„ï¼›å¦åˆ™ç”¨äº‘ç«¯IDå»ä¸‹è½½
    "model_path": MODEL_PATH, 

    "model_id_hf": "Qwen/Qwen1.5-0.5B",   # HuggingFace ID
    "model_id_ms": "qwen/Qwen1.5-0.5B",   # ModelScope ID (å¤‡ç”¨)
    
    # å®éªŒå‚æ•°
    "n_samples": 1000,                    # æœ¬æ¬¡å®éªŒä½¿ç”¨çš„æ ·æœ¬æ•°
    "n_val_samples": 20,                  # éªŒè¯é›†å¤§å°
    "poison_ratio": 0.3,                  # æŠ•æ¯’æ¯”ä¾‹
    "output_dir": os.path.join(CURRENT_DIR, "server_results"),
    "seed": 42
}

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# ==========================================
# 1. èµ„æºä¸‹è½½ä¸å‡†å¤‡ (çº¯å‡€ç‰ˆ - ä¸ä¾èµ– datasets åº“)
# ==========================================

def get_local_model_path():
    """
    æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æ¨¡å‹ï¼Œæ²¡æœ‰åˆ™ä¸‹è½½ (ä¼˜å…ˆå°è¯• ModelScopeï¼Œå…¶æ¬¡ HuggingFace)
    """
    if os.path.exists(os.path.join(MODEL_PATH, "config.json")):
        print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹å·²å­˜åœ¨: {MODEL_PATH}")
        return MODEL_PATH
    
    print(f"ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ï¼Œå¼€å§‹ä¸‹è½½...")
    
    # å°è¯• ModelScope
    try:
        print("ğŸš€ å°è¯•ä½¿ç”¨ ModelScope ä¸‹è½½...")
        from modelscope import snapshot_download
        mw_path = snapshot_download(CONFIG['model_id_ms'])
        
        if os.path.exists(MODEL_PATH):
            shutil.rmtree(MODEL_PATH)
        shutil.copytree(mw_path, MODEL_PATH)
        print(f"âœ… ModelScope ä¸‹è½½å®Œæˆ: {MODEL_PATH}")
        return MODEL_PATH
    except ImportError:
        print("âš ï¸ æœªå®‰è£… modelscope, è·³è¿‡ã€‚")
    except Exception as e:
        print(f"âŒ ModelScope ä¸‹è½½å¤±è´¥: {e}")

    # å°è¯• HuggingFace
    try:
        print("â˜ï¸ å°è¯•ä½¿ç”¨ HuggingFace ä¸‹è½½...")
        from huggingface_hub import snapshot_download
        snapshot_download(repo_id=CONFIG['model_id_hf'], local_dir=MODEL_PATH)
        return MODEL_PATH
    except Exception as e:
        print(f"âŒ HuggingFace ä¸‹è½½å¤±è´¥: {e}")
        raise RuntimeError("æ— æ³•ä¸‹è½½æ¨¡å‹ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½ Qwen1.5-0.5B åˆ° server_resources/qwen_model")

def prepare_data_local():
    """
    åŠ è½½æ•°æ®å¹¶è¿›è¡Œåˆ‡åˆ†ã€æŠ•æ¯’
    è¿”å›:
    1. final_data (æŠ•æ¯’åçš„è®­ç»ƒé›† -> å¯¹åº” 'Dirty Model')
    2. pure_data (æœªæŠ•æ¯’çš„çº¯å‡€è®­ç»ƒé›† -> å¯¹åº” 'Oracle Model')
    3. dirty_indices_gt (æŠ•æ¯’ç´¢å¼•)
    4. oracle_data (ç”¨äº Shapley è®¡ç®—çš„éªŒè¯é›†)
    """
    print("ğŸ“¥ æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")
    
    ds_full = None
    if os.path.exists(DATASET_PATH):
        try:
            with open(DATASET_PATH, 'r', encoding='utf-8') as f:
                ds_full = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½ JSON æ•°æ®: {len(ds_full)} æ¡")
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    if ds_full is None:
        print("â˜¢ï¸ æœªæ‰¾åˆ°æ•°æ®æˆ–åŠ è½½å¤±è´¥ï¼Œç”Ÿæˆåˆæˆæ•°æ®å…œåº•...")
        ds_full = [{"instruction": f"Solve {k}+{k}", "input":"", "output":f"{k+k}"} for k in range(5000)]

    # åˆ‡åˆ†å‰ n_samples
    current_n = CONFIG['n_samples']
    print(f"âœ‚ï¸ æˆªå–å‰ {current_n} æ¡æ•°æ®...")
    
    ds_list = []
    count = 0
    for item in ds_full:
        ds_list.append({
            "instruction": item.get("instruction", ""), 
            "input": item.get("input", ""), 
            "output": item.get("output", "")
        })
        count += 1
        if count >= current_n: break
    
    # è¿™æ˜¯ä¸€ä¸ªæ²¡æœ‰æŠ•æ¯’çš„çº¯å‡€å¤‡ä»½ï¼Œç”¨æ¥è®­ç»ƒ Oracle æ¨¡å‹
    pure_data = [x.copy() for x in ds_list] 
    
    # è¿™é‡Œçš„ oracle_data ä»…ç”¨äºè®¡ç®— Shapley æ—¶çš„â€œåŸºå‡†â€ï¼Œä¸å‚ä¸è®­ç»ƒ
    # æŒ‰ç…§æƒ¯ä¾‹ï¼Œæˆ‘ä»¬ä»å¹²å‡€æ•°æ®é‡Œç•™å‡ºä¸€å°éƒ¨åˆ†ä½œä¸ºéªŒè¯
    oracle_data = [x.copy() for x in ds_list[:100]] 
    
    # å¼€å§‹æŠ•æ¯’æ„é€  final_data
    final_data = []
    dirty_indices_gt = [] 
    
    set_seed(CONFIG['seed'])
    garbage_responses = ["I don't know.", "Error 404.", "Noise.", "Ignore."]
    
    print(f"ğŸ˜ˆ æ³¨å…¥å™ªå£° ({CONFIG['poison_ratio']:.0%})...")
    for i, item in enumerate(ds_list):
        is_poison = random.random() < CONFIG['poison_ratio']
        new_item = item.copy()
        if is_poison:
            new_item["output"] = random.choice(garbage_responses)
            dirty_indices_gt.append(i)
        final_data.append(new_item)
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæ¯•!")
    print(f"   - Dirty (è®­ç»ƒç”¨): {len(final_data)} æ¡ (æŠ•æ¯’ {len(dirty_indices_gt)})")
    print(f"   - Pure (å¯¹æ¯”ç”¨):  {len(pure_data)} æ¡")
    
    return final_data, pure_data, dirty_indices_gt, oracle_data

# ==========================================
# 2. æ¢¯åº¦ä¸ KNN-Shapley
# ==========================================

def compute_knn_shapley_gradient(train_grads, val_grads, K=10):
    """
    è®¡ç®— KNN-Shapley å€¼
    """
    N_train = train_grads.shape[0]
    N_val = val_grads.shape[0]
    
    # å½’ä¸€åŒ–
    train_grads = F.normalize(train_grads, p=2, dim=1)
    val_grads = F.normalize(val_grads, p=2, dim=1)
    
    print(f"   -> è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (CPU)...")
    val_cpu = val_grads.cpu()
    train_cpu = train_grads.cpu()
    S = torch.matmul(val_cpu, train_cpu.T).numpy()
    
    shapley_values = np.zeros(N_train)
    
    for j in range(N_val):
        s_row = S[j]
        topk_indices = np.argsort(s_row)[-K:]
        shapley_values[topk_indices] += s_row[topk_indices]
        
    shapley_values /= N_val
    return shapley_values

def extract_gradient_features(model_path, dataset_list, indices):
    print(f"ğŸ§¬ æå–æ¢¯åº¦... æ ·æœ¬æ•°: {len(indices)}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        device_map="auto", 
        torch_dtype=torch.float16, 
        trust_remote_code=True
    )
    
    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"])
    model = get_peft_model(model, peft_config)
    model.train()
    
    grads = []
    subset = [dataset_list[i] for i in indices]
    MAX_LEN = 256
    
    for item in tqdm(subset, desc="Grads"):
        text = f"User: {item['instruction']}\n{item['input']}\nAssistant: {item['output']}{tokenizer.eos_token}"
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_LEN).to(model.device)
        
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        loss.backward()
        
        g_vecs = []
        for name, param in model.named_parameters():
             if "lora" in name and param.grad is not None:
                g_vecs.append(param.grad.view(-1).cpu().float())
        
        if g_vecs:
            grads.append(torch.cat(g_vecs))
        else:
            grads.append(torch.zeros(1))
        model.zero_grad()
    
    del model
    torch.cuda.empty_cache()
    
    if not grads: return torch.zeros((len(indices), 1))
    return torch.stack(grads)

def calculate_shapley(model_path, dataset_list, oracle_data):
    n_oracle = min(len(oracle_data), CONFIG['n_val_samples'])
    oracle_subset = oracle_data[:n_oracle]
    
    print(f"ğŸ”§ Shapleyè®¡ç®—: Train={len(dataset_list)}, Val={len(oracle_subset)}")
    
    train_grads = extract_gradient_features(model_path, dataset_list, list(range(len(dataset_list))))
    val_grads = extract_gradient_features(model_path, oracle_subset, list(range(len(oracle_subset))))
    
    min_len = min(train_grads.shape[1], val_grads.shape[1])
    return compute_knn_shapley_gradient(train_grads[:, :min_len], val_grads[:, :min_len], K=5)

# ==========================================
# 3. è®­ç»ƒä¸è¯„ä¼°
# ==========================================

class SFTDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=256):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        item = self.data[index]
        text = f"User: {item['instruction']}\n{item['input']}\nAssistant: {item['output']}{self.tokenizer.eos_token}"
        tokenized = self.tokenizer(text, truncation=True, max_length=self.max_len, return_tensors=None)
        return {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": tokenized["input_ids"].copy()
        }

def run_sft_training(model_path, dataset_list, run_name):
    # å¦‚æœæ•°æ®é›†è¿‡å°ï¼Œè·³è¿‡
    if len(dataset_list) == 0:
        print(f"âš ï¸ {run_name} æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡è®­ç»ƒã€‚")
        return

    output_path = os.path.join(CONFIG['output_dir'], run_name)
    print(f"\nğŸš€ [Training] å¼€å§‹è®­ç»ƒ: {run_name}")
    print(f"   æ ·æœ¬æ•°é‡: {len(dataset_list)}")
    print(f"   ä¿å­˜è·¯å¾„: {output_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "right"

    # ä½¿ç”¨è‡ªå®šä¹‰ Dataset
    train_dataset = SFTDataset(dataset_list, tokenizer)
    
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", torch_dtype=torch.float32, trust_remote_code=True)
    model = get_peft_model(model, LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, target_modules=["q_proj", "v_proj"]))
    
    args = TrainingArguments(
        output_dir=output_path,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        num_train_epochs=3,      
        learning_rate=2e-4,
        logging_steps=10,
        save_strategy="no", # æœ¬åœ°DemoèŠ‚çœç©ºé—´ä¸ä¿å­˜æ¯è½®checkpoint
        report_to="none",
        fp16=False,
    )
    
    trainer = Trainer(
        model=model, 
        args=args, 
        train_dataset=train_dataset, 
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
    )
    trainer.train()
    
    # è®­ç»ƒç»“æŸåä¿å­˜ä¸€æ¬¡
    trainer.save_model(output_path)
    
    # --- ROUGE-L ç®€æ˜“è¯„ä¼° ---
    print(f"ğŸ“ [Eval] {run_name} ROUGE Check...")
    try:
        model.eval()
        # å–å‰ 10 ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•
        # ä¸ºäº†å…¬å¹³ï¼Œæˆ‘ä»¬åº”è¯¥ç”¨å›ºå®šçš„ã€å¹²å‡€çš„æµ‹è¯•é›†? 
        # è¿™é‡Œ Demo ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç”¨ dataset_list çš„å‰ 10 ä¸ªã€‚
        test_samples = dataset_list[:10]
        preds, refs = [], []
        
        def calculate_local_rouge(pred_str, ref_str):
            x = list(pred_str.strip())
            y = list(ref_str.strip())
            if not x or not y: return 0.0
            m, n = len(x), len(y)
            dp = [[0] * (n + 1) for _ in range(m + 1)]
            for i in range(1, m + 1):
                for j in range(1, n + 1):
                    if x[i - 1] == y[j - 1]:
                        dp[i][j] = dp[i - 1][j - 1] + 1
                    else:
                        dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
            lcs_len = dp[m][n]
            if (len(x) + len(y)) == 0: return 0.0
            return 2.0 * lcs_len / (len(x) + len(y))

        for item in test_samples:
            prompt = f"User: {item['instruction']}\n\nAssistant: "
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)
            pred = tokenizer.decode(outputs[0], skip_special_tokens=True).split("Assistant: ")[-1].strip()
            preds.append(pred)
            refs.append(item['output'])
        
        scores = [calculate_local_rouge(p, r) for p, r in zip(preds, refs)]
        avg_score = sum(scores) / len(scores) if scores else 0.0
        print(f"ğŸ“Š {run_name} Avg ROUGE-L: {avg_score:.4f}")
        
    except Exception as e:
        print(f"âš ï¸ Eval Error: {e}")
    
    # æ¸…ç†æ˜¾å­˜
    del model, trainer
    torch.cuda.empty_cache()

# ==========================================
# ä¸»æµç¨‹
# ==========================================
def main():
    print(f"ğŸŒŸ SFT Server Persistent Demo (Multi-Model Comparison) å¯åŠ¨")
    
    model_path = get_local_model_path()
    
    # 1. å‡†å¤‡æ•°æ®
    # raw_dirty: åŒ…å«æŠ•æ¯’çš„æ•°æ® (å¯¹åº” Baseline)
    # raw_pure:  åŸæœ¬çš„å¹²å‡€æ•°æ® (å¯¹åº” Oracle)
    raw_dirty, raw_pure, dirty_indices_gt, oracle_data = prepare_data_local()
    
    # 2. è®¡ç®— Shapley å¹¶æ¸…æ´—
    # è®¡ç®—æ˜¯åŸºäº raw_dirty è¿›è¡Œç­›é€‰
    sv = calculate_shapley(model_path, raw_dirty, oracle_data)
    
    n_remove = int(len(raw_dirty) * CONFIG['poison_ratio'])
    keep_indices = np.argsort(sv)[n_remove:]
    # cleaned_data: ç®—æ³•æ¸…æ´—åçš„æ•°æ® (å¯¹åº” Clean/Ours)
    cleaned_data = [raw_dirty[i] for i in keep_indices]
    
    # 3. è®¡ç®— Recall
    removed_indices = np.argsort(sv)[:n_remove]
    recall = len(set(removed_indices).intersection(set(dirty_indices_gt))) / (len(dirty_indices_gt) + 1e-9)
    print(f"âœ… Shapley æ¸…æ´— Recall: {recall:.2%}")

    # 4. å¯¹æ¯”è®­ç»ƒ
    print("\nâš”ï¸ å¼€å§‹ä¸‰ç»„æ¨¡å‹å¯¹æ¯”è®­ç»ƒ âš”ï¸")
    print("------------------------------------------------")
    
    # A. è„æ¨¡å‹ (Dirty Model) - ç”¨è¢«æŠ•æ¯’çš„æ•°æ®ç»ƒ
    run_sft_training(model_path, raw_dirty, "dirty_model")
    
    # B. ç†æƒ³æ¨¡å‹ (Oracle Model) - ç”¨æœªæ‹†å°çš„å¹²å‡€æ•°æ®ç»ƒ (ä¸Šé™)
    run_sft_training(model_path, raw_pure, "oracle_model")
    
    # C. æˆ‘ä»¬çš„æ¨¡å‹ (Clean Model) - ç”¨ Shapley æ´—è¿‡çš„æ•°æ®ç»ƒ
    run_sft_training(model_path, cleaned_data, "clean_model")
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ! è¯·æŸ¥çœ‹ä¸Šæ–¹çš„ ROUGE åˆ†æ•°å·®å¼‚ã€‚")

if __name__ == "__main__":
    main()
