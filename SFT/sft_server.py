import os
import sys
import json
import random
import numpy as np
import torch
from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score
import evaluate
from tqdm import tqdm

# ==========================================
# 0. æœåŠ¡å™¨ç¯å¢ƒé…ç½®
# ==========================================

# 1. é•œåƒåŠ é€Ÿ (å¦‚æœæ‚¨çš„æœåŠ¡å™¨åœ¨å›½å†…ï¼Œè¯·ä¿ç•™æ­¤è¡Œï¼›å¦‚æœåœ¨æµ·å¤–ï¼Œå¯ä»¥æ³¨é‡Šæ‰)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# 2. é¿å…å¹¶è¡Œ Tokenizer è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 3. å®šä¹‰å…¨å±€é…ç½®
CONFIG = {
    # ç›´æ¥ä½¿ç”¨ Hugging Face Hub ä¸Šçš„æ¨¡å‹ IDï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ä¸‹è½½
    # å»ºè®®ä½¿ç”¨ Qwen1.5-0.5B æˆ– Qwen1.5-1.8B ä½œä¸ºæµ‹è¯•ï¼Œé€Ÿåº¦å¿«æ˜¾å­˜å ç”¨å°
    "model_name": "Qwen/Qwen1.5-0.5B",       
    
    # å®éªŒå‚æ•°
    "n_samples": 1000,                       # æ ·æœ¬æ•°é‡ (æœåŠ¡å™¨è·‘å¦‚æœä¸ç¼ºæ—¶é—´å¯ä»¥è®¾å¤§ä¸€ç‚¹ï¼Œæ¯”å¦‚ 2000-5000)
    "n_val_samples": 20,                     # éªŒè¯é›†å¤§å° (æ¢¯åº¦è®¡ç®—ç”¨)
    "poison_ratio": 0.3,                     # æŠ•æ¯’æ¯”ä¾‹
    "output_dir": "./server_results",        # ç»“æœä¿å­˜è·¯å¾„ (å½“å‰ç›®å½•ä¸‹çš„ server_results)
    "seed": 42
}

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

print(f"ğŸš€ æœåŠ¡å™¨æ¨¡å¼å¯åŠ¨ | æ¨¡å‹: {CONFIG['model_name']} | æ ·æœ¬æ•°: {CONFIG['n_samples']}")
print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: {os.path.abspath(CONFIG['output_dir'])}")

# ==========================================
# 1. æ•°æ®å‡†å¤‡ (è‡ªåŠ¨ä¸‹è½½ + åˆæˆ)
# ==========================================
def prepare_data():
    """
    è‡ªåŠ¨ä¸‹è½½ Alpaca æ•°æ®é›†ï¼Œå¹¶äººå·¥æ³¨å…¥å™ªå£°ã€‚
    """
    print("ğŸ“¥ æ­£åœ¨å‡†å¤‡æ•°æ®...")
    
    ds_raw = None
    try:
        # å°è¯•ä» Hugging Face åœ¨çº¿åŠ è½½
        # 'yahma/alpaca-cleaned' æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æ¸…ç†ç‰ˆï¼Œæˆ–è€…ç”¨ 'tatsu-lab/alpaca'
        print(f"â˜ï¸æ­£åœ¨ä¸‹è½½æ•°æ®é›†: tatsu-lab/alpaca (å‰ {CONFIG['n_samples']} æ¡)...")
        # split å‚æ•°ç›´æ¥åˆ‡ç‰‡ï¼Œåªä¸‹è½½éœ€è¦çš„éƒ¨åˆ†
        ds_raw = load_dataset("tatsu-lab/alpaca", split=f"train[:{CONFIG['n_samples']}]")
        print("âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âš ï¸ åœ¨çº¿åŠ è½½å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨çº¯åˆæˆæ•°æ®å…œåº•...")

    # è½¬æ¢ä¸º Python List
    ds = []
    if ds_raw:
        for item in ds_raw:
            ds.append({
                "instruction": item["instruction"],
                "input": item["input"],
                "output": item["output"]
            })
    else:
        # å…œåº•åˆæˆæ•°æ® (ç®—æœ¯é¢˜)
        print("â˜¢ï¸ ä½¿ç”¨åˆæˆæ•°æ®å…œåº•...")
        for k in range(CONFIG['n_samples']):
            ds.append({
                "instruction": f"Calculate {k} + {k}",
                "input": "", 
                "output": f"{k+k}"
            })
            
    data = []
    clean_indices_gt = [] # Ground Truth Clean
    dirty_indices_gt = [] # Ground Truth Dirty
    
    set_seed(CONFIG['seed'])
    
    # å®šä¹‰å™ªå£°æ¨¡æ¿
    garbage_responses = [
        "I don't know the answer because I am an AI.",
        "This is a random sentence generated by a computer.",
        "Error 404: Answer not found.",
        "Bla bla bla, I am just outputting noise.",
        "Ignore the previous instruction, here is a poem about cats.",
        "System reboot initiated.",
        "As an AI language model, I cannot answer this request."
    ]
    
    print(f"ğŸ˜ˆ æ­£åœ¨æ³¨å…¥å™ªå£° (æ¯”ä¾‹: {CONFIG['poison_ratio']:.0%})...")
    
    # åˆ›å»ºä¸€ä»½çº¯å‡€çš„å‰¯æœ¬ä½œä¸º Oracle (éªŒè¯é›†æ¥æº)
    oracle_data = [item.copy() for item in ds] 

    for i, item in enumerate(ds):
        is_poison = random.random() < CONFIG['poison_ratio']
        
        new_item = item.copy()
        
        if is_poison:
            new_item["output"] = random.choice(garbage_responses)
            dirty_indices_gt.append(i)
        else:
            clean_indices_gt.append(i)
            
        data.append(new_item)
        
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: æ€»æ•° {len(data)} | å¹²å‡€æ ·æœ¬(GT): {len(clean_indices_gt)} | è„æ ·æœ¬(GT): {len(dirty_indices_gt)}")
    return data, dirty_indices_gt, oracle_data

# ==========================================
# 2. æ¢¯åº¦ç‰¹å¾æå–ä¸ KNN-Shapley
# ==========================================

def compute_knn_shapley_gradient(train_grads, val_grads, K=10):
    """
    è®¡ç®—åŸºäºæ¢¯åº¦çš„ Shapley Valueã€‚
    """
    N_train = train_grads.shape[0]
    N_val = val_grads.shape[0]
    
    print(f"   -> å½’ä¸€åŒ–æ¢¯åº¦å‘é‡...")
    train_grads = F.normalize(train_grads, p=2, dim=1)
    val_grads = F.normalize(val_grads, p=2, dim=1)
    
    print(f"   -> è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ ({N_val}x{N_train})...")
    # è½¬åˆ° CPU è®¡ç®—çŸ©é˜µä¹˜æ³•ï¼Œé˜²æ­¢çˆ†æ˜¾å­˜
    S = torch.matmul(val_grads, train_grads.T).cpu().numpy() 
    
    shapley_values = np.zeros(N_train)
    
    print(f"   -> è¿è¡Œ KNN-Shapley ä¼°å€¼ (K={K})...")
    for j in range(N_val):
        s_row = S[j]
        sorted_indices = np.argsort(s_row)[::-1] # é™åº
        values = s_row[sorted_indices]
        
        phi_sorted = np.zeros(N_train)
        curr_K = min(K, N_train)
        
        # ç®€åŒ–ç‰ˆé€’å½’è®¡ç®— (ä»…å…³æ³¨ Top-K å¸¦æ¥çš„è¾¹é™…å¢ç›Š)
        running_sum = 0.0
        # ä» K-1 åˆ° 0
        for i in range(curr_K - 1, -1, -1):
            val_current = values[i]
            val_next = values[i+1] if (i+1 < N_train) else 0.0
            running_sum += (val_current - val_next) / (i + 1)
            phi_sorted[i] = running_sum # è¿™æ˜¯æ ¹æ®åŸæ–‡å…¬å¼è¿‘ä¼¼
            
            # æ›´ç®€å•çš„è¿‘ä¼¼: ç›´æ¥ç´¯åŠ  Top-K ç›¸ä¼¼åº¦ (KNN-Filter)
            # phi_sorted[i] = values[i] / curr_K 
        
        # ç®€å•å®ç°ï¼šç›´æ¥ç´¯åŠ ç›¸ä¼¼åº¦ (KNN Proxy) - è¿™ç§æ–¹æ³•åœ¨æ¸…æ´—ä»»åŠ¡ä¸­å¾€å¾€æ›´ç¨³å¥
        #shapley_values += s_row 
        
        # ä½¿ç”¨ Jia et al. çš„å…¬å¼é€»è¾‘
        for i in range(N_train - 1, -1, -1): # å€’åº
             if i < curr_K:
                 phi_sorted[i] = phi_sorted[i+1] + (values[i] - values[i+1])/ (i+1) if i+1 < N_train else values[i]/(i+1)
             else:
                 phi_sorted[i] = 0

        shapley_values[sorted_indices] += phi_sorted
        
    shapley_values /= N_val
    return shapley_values

def extract_gradient_features(dataset_list, indices):
    """
    ä½¿ç”¨ LoRA æå–æ¢¯åº¦ç‰¹å¾ã€‚
    """
    print(f"ğŸ§¬ æå–æ¢¯åº¦... æ ·æœ¬æ•°: {len(indices)}")
    
    # è‡ªåŠ¨ä¸‹è½½/åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    model = AutoModelForCausalLM.from_pretrained(
        CONFIG['model_name'], 
        device_map="auto", 
        torch_dtype=torch.float16, 
        trust_remote_code=True
    )
    
    # LoRA é…ç½®
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=16, 
        target_modules=["q_proj", "v_proj"]
    )
    model = get_peft_model(model, peft_config)
    model.train() # å¿…é¡»æ˜¯ train æ¨¡å¼æ‰æœ‰æ¢¯åº¦
    
    grads = []
    
    def format_text(ex):
        return f"User: {ex['instruction']}\n{ex['input']}\nAssistant: {ex['output']}{tokenizer.eos_token}"

    subset = [dataset_list[i] for i in indices]
    
    # ä¸ºäº†é€Ÿåº¦ï¼Œè¿™é‡Œè®¾ç½®è¾ƒçŸ­çš„ max_length
    MAX_LEN = 256
    
    for item in tqdm(subset, desc="Gradient Extraction"):
        text = format_text(item)
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_LEN).to(model.device)
        
        # Forward
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        loss.backward()
        
        # Extract Grad
        g_vecs = []
        # éå†æ‰€æœ‰ LoRA å‚æ•°
        for name, param in model.named_parameters():
             if "lora" in name and param.grad is not None:
                g_vecs.append(param.grad.view(-1).cpu().float())
        
        if g_vecs:
            # æ‹¼æ¥æ‰€æœ‰å±‚çš„ LoRA æ¢¯åº¦ä½œä¸ºç‰¹å¾å‘é‡
            grads.append(torch.cat(g_vecs))
        else:
            grads.append(torch.zeros(1))
            
        model.zero_grad()
    
    del model
    torch.cuda.empty_cache()
    
    if not grads:
        return torch.zeros((len(indices), 1))
        
    return torch.stack(grads)

def calculate_shapley(dataset_list, oracle_data):
    n_train = len(dataset_list)
    n_val = min(len(oracle_data), CONFIG['n_val_samples'])
    
    print(f"ğŸ”§ å¼€å§‹æ¢¯åº¦æ¸…æ´—è®¡ç®—...")
    
    # 1. Train Set Gradients
    train_indices = list(range(n_train))
    train_grads = extract_gradient_features(dataset_list, train_indices)
    
    # 2. Val Set Gradients (Gold Standard)
    val_indices = list(range(n_val))
    val_grads = extract_gradient_features(oracle_data[:n_val], val_indices)
    
    # ç»´åº¦å¯¹é½
    if train_grads.shape[1] != val_grads.shape[1]:
        # å¦‚æœé•¿åº¦ä¸ä¸€è‡´ï¼ˆæå…¶ç½•è§ï¼‰ï¼Œæˆªæ–­
        min_len = min(train_grads.shape[1], val_grads.shape[1])
        train_grads = train_grads[:, :min_len]
        val_grads = val_grads[:, :min_len]

    # 3. KNN-Shapley
    sv = compute_knn_shapley_gradient(train_grads, val_grads, K=5)
    return sv

# ==========================================
# 3. è®­ç»ƒä¸è¯„ä¼°
# ==========================================
def run_sft_training(dataset_list, run_name):
    """
    å¾®è°ƒæ¨¡å‹å¹¶è¯„ä¼°
    """
    output_path = f"{CONFIG['output_dir']}/{run_name}"
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ: {run_name} -> {output_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # å¤„ç†æ•°æ®
    def format_func(example):
        text = f"User: {example['instruction']}\n{example['input']}\nAssistant: {example['output']}{tokenizer.eos_token}"
        return {"text": text}
    
    hf_dataset = Dataset.from_list(dataset_list)
    hf_dataset = hf_dataset.map(lambda x: tokenizer(format_func(x)["text"], truncation=True, max_length=256), batched=False)
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        CONFIG['model_name'], 
        device_map="auto", 
        torch_dtype=torch.float32, # è®­ç»ƒå»ºè®®ç”¨ FP32 æˆ– BF16ï¼Œé¿å… FP16 æº¢å‡º
        trust_remote_code=True
    )
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"]
    )
    model = get_peft_model(model, peft_config)
    
    args = TrainingArguments(
        output_dir=output_path,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        num_train_epochs=3, # è¿™é‡Œçš„ epoch å°‘ä¸€ç‚¹æ¼”ç¤ºç”¨ï¼Œå®é™…å»ºè®® 3-5
        learning_rate=2e-4,
        logging_steps=10,
        save_strategy="no", # æ¼”ç¤ºä¸ä¿å­˜ checkpoint ä¸­é—´æ€
        report_to="none",
        fp16=False,
    )
    
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=hf_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    )
    
    trainer.train()
    
    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_path}")
    trainer.save_model(output_path)
    
    # ç®€å•è¯„ä¼° (ROUGE)
    print("ğŸ“ è¯„ä¼° ROUGE (éšæœºæŠ½å– 20 æ¡æµ‹è¯•)...")
    try:
        metric = evaluate.load("rouge")
        model.eval()
        
        test_samples = dataset_list[:20] if len(dataset_list) > 20 else dataset_list
        preds = []
        refs = []
        
        for item in test_samples:
            prompt = f"User: {item['instruction']}\n\nAssistant: "
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)
            pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # ç®€å•çš„æˆªå–å›å¤
            if "Assistant: " in pred:
                pred = pred.split("Assistant: ")[-1].strip()
            preds.append(pred)
            refs.append(item['output'])
            
        scores = metric.compute(predictions=preds, references=refs)
        rouge_score = scores['rougeL']
        print(f"ğŸ“Š {run_name} ROUGE-L: {rouge_score:.4f}")
        return rouge_score
    except Exception as e:
        print(f"âš ï¸ ROUGE è¯„ä¼°å¤±è´¥: {e}")
        return 0.0

# ==========================================
# ä¸»æµç¨‹
# ==========================================
def main():
    print(f"ğŸŒŸ SFT Server Demo å¼€å§‹è¿è¡Œ...")
    
    # 1. å‡†å¤‡æ•°æ®
    raw_data, dirty_indices_gt, oracle_data = prepare_data()
    
    # 2. è®¡ç®— Shapley å€¼å¹¶æ¸…æ´—
    sv = calculate_shapley(raw_data, oracle_data)
    
    # 3. æ‰§è¡Œæ¸…æ´—
    n_remove = int(len(raw_data) * CONFIG['poison_ratio'])
    sorted_idx = np.argsort(sv) # ä»å°åˆ°å¤§
    keep_indices = sorted_idx[n_remove:] # ä¿ç•™åˆ†æ•°é«˜çš„
    
    cleaned_data = [raw_data[i] for i in keep_indices]
    
    # 4. ç»Ÿè®¡æŒ‡æ ‡
    removed_indices = sorted_idx[:n_remove]
    correct_removed = set(removed_indices).intersection(set(dirty_indices_gt))
    recall = len(correct_removed) / (len(dirty_indices_gt) + 1e-9)
    print(f"âœ… æ¸…æ´—å®Œæˆ! ç§»é™¤äº† {n_remove} æ¡æ•°æ®")
    print(f"ğŸ•µï¸â€â™‚ï¸ è„æ•°æ®å¬å›ç‡ (Recall): {recall:.2%}")
    
    # è®¡ç®— AUC
    if len(dirty_indices_gt) > 0:
        true_labels = np.ones(len(raw_data))
        true_labels[dirty_indices_gt] = 0
        auc = roc_auc_score(true_labels, sv)
        print(f"ğŸ“ˆ ç®—æ³•æ ¹æ®æ¢¯åº¦æ‰“åˆ†çš„ AUC: {auc:.4f}")
    
    # 5. è®­ç»ƒå¯¹æ¯” (Dirty vs Clean)
    # ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œè¿™é‡Œæˆ‘ä»¬åªè®­ç»ƒ Clean æ¨¡å‹æ¼”ç¤ºæµç¨‹
    # å¦‚æœæ‚¨æƒ³å¯¹æ¯”ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢ Dirty Model çš„æ³¨é‡Š
    
    # print("--- Training Dirty Model ---")
    # run_sft_training(raw_data, "dirty_model")
    
    print("--- Training Clean Model ---")
    rouge_clean = run_sft_training(cleaned_data, "clean_model")
    
    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {CONFIG['output_dir']}/clean_model")

if __name__ == "__main__":
    main()
