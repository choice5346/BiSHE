import os
import sys
import json
import random
import numpy as np
import torch
import shutil
from datasets import load_dataset, Dataset, load_from_disk
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score
import evaluate
from tqdm import tqdm

# ==========================================
# 0. ç¯å¢ƒä¸è·¯å¾„é…ç½®
# ==========================================

# è®¾ç½® HF é•œåƒ (é’ˆå¯¹å›½å†…ç½‘ç»œ)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å®šä¹‰æœ¬åœ°èµ„æºä¿å­˜è·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# èµ„æºä¿å­˜åœ¨å½“å‰è„šæœ¬åŒçº§ç›®å½•ä¸‹çš„ server_resources
RESOURCES_DIR = os.path.join(CURRENT_DIR, "server_resources")
DATASET_PATH = os.path.join(RESOURCES_DIR, "alpaca_data")
MODEL_PATH = os.path.join(RESOURCES_DIR, "qwen_model")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(RESOURCES_DIR, exist_ok=True)

CONFIG = {
    # è‡ªåŠ¨è¯†åˆ«ï¼šå¦‚æœæœ¬åœ° MODEL_PATH é‡Œæœ‰æ–‡ä»¶ï¼Œå°±ç”¨æœ¬åœ°è·¯å¾„ï¼›å¦åˆ™ç”¨äº‘ç«¯IDå»ä¸‹è½½
    "model_path": MODEL_PATH, 

    "model_id_hf": "Qwen/Qwen1.5-0.5B",   # HuggingFace ID
    "model_id_ms": "qwen/Qwen1.5-0.5B",   # ModelScope ID (å¤‡ç”¨)
    
    # å®éªŒå‚æ•°
    "n_samples": 1000,                    # æœ¬æ¬¡å®éªŒä½¿ç”¨çš„æ ·æœ¬æ•°
    "n_val_samples": 20,                  # éªŒè¯é›†å¤§å°
    "poison_ratio": 0.3,                  # æŠ•æ¯’æ¯”ä¾‹
    "output_dir": os.path.join(CURRENT_DIR, "server_results"),
    "seed": 42
}

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# ==========================================
# 1. èµ„æºä¸‹è½½ä¸å‡†å¤‡ (æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†)
# ==========================================

def get_local_model_path():
    """
    æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æ¨¡å‹ï¼Œæ²¡æœ‰åˆ™ä¸‹è½½ (ä¼˜å…ˆå°è¯• ModelScopeï¼Œå…¶æ¬¡ HuggingFace)
    """
    # 1. æ£€æŸ¥æŒ‡å®šç›®å½•ä¸‹æ˜¯å¦æœ‰ config.jsonï¼Œå¦‚æœæœ‰è¯´æ˜å·²ç»ä¸‹è½½è¿‡äº†
    if os.path.exists(os.path.join(MODEL_PATH, "config.json")):
        print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹å·²å­˜åœ¨: {MODEL_PATH}")
        return MODEL_PATH
    
    print(f"ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ï¼Œå¼€å§‹ä¸‹è½½...")
    print(f"   ç›®æ ‡è·¯å¾„: {MODEL_PATH}")

    # 2. å°è¯•ä½¿ç”¨ ModelScope ä¸‹è½½ (å›½å†…æœ€å¿«)
    try:
        print("ğŸš€ å°è¯•ä½¿ç”¨ ModelScope ä¸‹è½½ (å›½å†…æ¨è)...")
        from modelscope import snapshot_download
        # ModelScope ä¸‹è½½åä¼šè¿”å›å…·ä½“çš„ç¼“å­˜è·¯å¾„
        mw_path = snapshot_download(CONFIG['model_id_ms'])
        
        # å°†ä¸‹è½½çš„æ–‡ä»¶å¤åˆ¶/ç§»åŠ¨åˆ°æˆ‘ä»¬æŒ‡å®šçš„ MODEL_PATH
        print(f"   ModelScope ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨åŒæ­¥åˆ° {MODEL_PATH} ...")
        
        # å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨ä¸”éç©ºï¼Œå…ˆæ¸…ç©º
        if os.path.exists(MODEL_PATH):
            shutil.rmtree(MODEL_PATH)
        
        # å¤åˆ¶
        shutil.copytree(mw_path, MODEL_PATH)
        print(f"âœ… æ¨¡å‹å·²å°±ç»ª: {MODEL_PATH}")
        return MODEL_PATH
    except ImportError:
        print("âš ï¸ æœªå®‰è£… modelscope åº“ï¼Œè·³è¿‡ ModelScope ä¸‹è½½æ–¹å¼ã€‚(å»ºè®® pip install modelscope)")
    except Exception as e:
        print(f"âŒ ModelScope ä¸‹è½½å¤±è´¥: {e}")

    # 3. å°è¯•ä½¿ç”¨ HuggingFace ä¸‹è½½ (ä½¿ç”¨é•œåƒ)
    try:
        print("â˜ï¸ å°è¯•ä½¿ç”¨ HuggingFace (hf-mirror) ä¸‹è½½...")
        from huggingface_hub import snapshot_download
        snapshot_download(
            repo_id=CONFIG['model_id_hf'],
            local_dir=MODEL_PATH,
            local_dir_use_symlinks=False,  
            resume_download=True
        )
        return MODEL_PATH
    except Exception as e:
        print(f"âŒ HuggingFace ä¸‹è½½å¤±è´¥: {e}")
        raise RuntimeError("æ— æ³•ä¸‹è½½æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° server_resources/qwen_model ç›®å½•")

def prepare_data_local():
    """
    æ•°æ®æœ¬åœ°åŒ–åŠ è½½é€»è¾‘
    """
    print("ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®...")
    ds_full = None
    
    # 1. ä¼˜å…ˆåŠ è½½æœ¬åœ°
    if os.path.exists(DATASET_PATH):
        try:
            print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®é›†: {DATASET_PATH}")
            from datasets import load_from_disk
            ds_loaded = load_from_disk(DATASET_PATH)
            if isinstance(ds_loaded, dict) or hasattr(ds_loaded, 'keys'):
                ds_full = ds_loaded['train'] if 'train' in ds_loaded else list(ds_loaded.values())[0]
            else:
                ds_full = ds_loaded
            print(f"âœ… æœ¬åœ°æ•°æ®åŠ è½½æˆåŠŸ! æ€»é‡: {len(ds_full)}")
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ•°æ®æŸå: {e}")
            
    # 2. å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•ä¸‹è½½ (ä¼˜å…ˆ ModelScope/HF)
    if ds_full is None:
        print("â˜ï¸ æ­£åœ¨ä¸‹è½½ tatsu-lab/alpaca æ•°æ®é›†...")
        
        # --- æ–¹æ¡ˆ A: ä½¿ç”¨ ModelScope ä¸‹è½½ (å›½å†…æœ€å¿«) ---
        try:
            print("   [Attempt 1] å°è¯• ModelScope (AI-ModelScope/alpaca-gpt4-data-en)...")
            from modelscope.msdatasets import MsDataset
            # ModelScope ä¸Šçš„ Alpaca æ•°æ®é›† (è‹±æ–‡ç‰ˆ)
            ms_ds = MsDataset.load('AI-ModelScope/alpaca-gpt4-data-en', split='train')
            # è½¬æ¢ä¸º HuggingFace æ ¼å¼ List[Dict]
            ds_full = []
            print("   -> æ­£åœ¨è½¬æ¢æ•°æ®æ ¼å¼...")
            for item in ms_ds:
                ds_full.append({
                    'instruction': item.get('instruction', ''),
                    'input': item.get('input', ''),
                    'output': item.get('output', '')
                })
            print(f"âœ… ModelScope ä¸‹è½½å¹¶è½¬æ¢æˆåŠŸ! æ¡æ•°: {len(ds_full)}")
        except Exception as e:
            print(f"âš ï¸ ModelScope ä¸‹è½½å¤±è´¥: {e}")

        # --- æ–¹æ¡ˆ B: ä½¿ç”¨ HF é•œåƒä¸‹è½½ (å¤‡é€‰) ---
        if ds_full is None:
            try:
                print("   [Attempt 2] å°è¯• HuggingFace é•œåƒ (hf-mirror.com)...")
                # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶èµ°æ­¤é•œåƒ
                os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
                ds_full = load_dataset("tatsu-lab/alpaca", split="train")
                print("âœ… HF é•œåƒä¸‹è½½æˆåŠŸ!")
            except Exception as e:
                print(f"âš ï¸ HF é•œåƒä¸‹è½½å¤±è´¥: {e}")
        
        # --- ä¿å­˜åˆ°æœ¬åœ° ---
        if ds_full is not None:
            try:
                print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®é›†åˆ°æœ¬åœ°: {DATASET_PATH} ...")
                # å¦‚æœæ˜¯ Listï¼Œå…ˆè½¬ Dataset
                if isinstance(ds_full, list):
                    Dataset.from_list(ds_full).save_to_disk(DATASET_PATH)
                else:
                    ds_full.save_to_disk(DATASET_PATH)
                print("âœ… æ•°æ®é›†å·²æŒä¹…åŒ–ä¿å­˜ã€‚")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")
        
        # --- æœ€ç»ˆå…œåº• ---
        if ds_full is None:
            print("â˜¢ï¸ æ‰€æœ‰ä¸‹è½½æ–¹å¼å‡å¤±è´¥ï¼Œä½¿ç”¨åˆæˆæ•°æ®å…œåº•...")
            ds_full = [{"instruction": f"Solve {k}+{k}", "input":"", "output":f"{k+k}"} for k in range(5000)]

    # 3. åˆ‡åˆ†æ•°æ®
    current_n = CONFIG['n_samples']
    print(f"âœ‚ï¸ æ­£åœ¨æˆªå–å‰ {current_n} æ¡æ•°æ®ç”¨äºæœ¬æ¬¡å®éªŒ...")
    
    ds_list = []
    count = 0
    for item in ds_full:
        ds_list.append({"instruction": item["instruction"], "input": item["input"], "output": item["output"]})
        count += 1
        if count >= current_n: break
            
    # 4. æŠ•æ¯’
    final_data = []
    dirty_indices_gt = [] 
    
    # Oracle ä»åŸå§‹æ•°æ®é‡Œå– 100 æ¡ (ä¸æŠ•æ¯’)
    # ç¡®ä¿ oracle_data ä¸å—æŠ•æ¯’å½±å“
    oracle_data = [x.copy() for x in ds_list[:100]] 
    
    set_seed(CONFIG['seed'])
    garbage_responses = ["I don't know.", "Error 404.", "Noise.", "Ignore."]
    
    print(f"ğŸ˜ˆ æ³¨å…¥å™ªå£° ({CONFIG['poison_ratio']:.0%})...")
    for i, item in enumerate(ds_list):
        is_poison = random.random() < CONFIG['poison_ratio']
        new_item = item.copy()
        if is_poison:
            new_item["output"] = random.choice(garbage_responses)
            dirty_indices_gt.append(i)
        final_data.append(new_item)
    
    print(f"âœ… æœ€ç»ˆè®­ç»ƒæ•°æ®: {len(final_data)} æ¡ | éªŒè¯æ•°æ®(Oracle): {len(oracle_data)} æ¡")
    return final_data, dirty_indices_gt, oracle_data

# ==========================================
# 2. æ¢¯åº¦ä¸ KNN-Shapley (ä¼˜åŒ–ç‰ˆ)
# ==========================================

def compute_knn_shapley_gradient(train_grads, val_grads, K=10):
    N_train = train_grads.shape[0]
    N_val = val_grads.shape[0]
    
    print(f"   -> å½’ä¸€åŒ–...")
    train_grads = F.normalize(train_grads, p=2, dim=1)
    val_grads = F.normalize(val_grads, p=2, dim=1)
    
    print(f"   -> è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (CPU)...")
    # ç§»è‡³ CPU è®¡ç®—é¿å… OOM
    val_cpu = val_grads.cpu()
    train_cpu = train_grads.cpu()
    S = torch.matmul(val_cpu, train_cpu.T).numpy()
    
    shapley_values = np.zeros(N_train)
    
    print(f"   -> KNN ä¼°å€¼...")
    for j in range(N_val):
        s_row = S[j]
        topk_indices = np.argsort(s_row)[-K:]
        shapley_values[topk_indices] += s_row[topk_indices]
        
    shapley_values /= N_val
    return shapley_values

def extract_gradient_features(model_path, dataset_list, indices):
    print(f"ğŸ§¬ æå–æ¢¯åº¦... æ ·æœ¬æ•°: {len(indices)}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        device_map="auto", 
        torch_dtype=torch.float16, 
        trust_remote_code=True
    )
    
    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"])
    model = get_peft_model(model, peft_config)
    model.train()
    
    grads = []
    subset = [dataset_list[i] for i in indices]
    MAX_LEN = 256
    
    for item in tqdm(subset, desc="Grads"):
        text = f"User: {item['instruction']}\n{item['input']}\nAssistant: {item['output']}{tokenizer.eos_token}"
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_LEN).to(model.device)
        
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        loss.backward()
        
        g_vecs = []
        for name, param in model.named_parameters():
             if "lora" in name and param.grad is not None:
                g_vecs.append(param.grad.view(-1).cpu().float())
        
        if g_vecs:
            grads.append(torch.cat(g_vecs))
        else:
            grads.append(torch.zeros(1))
        model.zero_grad()
    
    del model
    torch.cuda.empty_cache()
    
    if not grads: return torch.zeros((len(indices), 1))
    return torch.stack(grads)

def calculate_shapley(model_path, dataset_list, oracle_data):
    # ç¡®ä¿ oracle_data ä¸ä¼šå¤ªå¤šæŠŠå†…å­˜æ’‘çˆ†
    n_oracle = min(len(oracle_data), CONFIG['n_val_samples'])
    oracle_subset = oracle_data[:n_oracle]
    
    print(f"ğŸ”§ å‡†å¤‡è®¡ç®—: Train={len(dataset_list)}, Val={len(oracle_subset)}")
    
    train_grads = extract_gradient_features(model_path, dataset_list, list(range(len(dataset_list))))
    val_grads = extract_gradient_features(model_path, oracle_subset, list(range(len(oracle_subset))))
    
    min_len = min(train_grads.shape[1], val_grads.shape[1])
    return compute_knn_shapley_gradient(train_grads[:, :min_len], val_grads[:, :min_len], K=5)

# ==========================================
# 3. è®­ç»ƒä¸è¯„ä¼°
# ==========================================
def run_sft_training(model_path, dataset_list, run_name):
    output_path = os.path.join(CONFIG['output_dir'], run_name)
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ: {run_name} -> {output_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    hf_dataset = Dataset.from_list(dataset_list)
    hf_dataset = hf_dataset.map(lambda x: tokenizer(f"User: {x['instruction']}\n{x['input']}\nAssistant: {x['output']}{tokenizer.eos_token}", truncation=True, max_length=256), batched=False)
    
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", torch_dtype=torch.float32, trust_remote_code=True)
    model = get_peft_model(model, LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, target_modules=["q_proj", "v_proj"]))
    
    args = TrainingArguments(
        output_dir=output_path,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-4,
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=1,
        report_to="none",
        fp16=False,
    )
    
    trainer = Trainer(model=model, args=args, train_dataset=hf_dataset, data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False))
    trainer.train()
    trainer.save_model(output_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    
    # ç®€å•çš„ ROUGE è¯„ä¼° (æœ¬åœ°ç¦»çº¿ç‰ˆ - æ— éœ€è”ç½‘)
    print("ğŸ“ ROUGE Check (Offline)...")
    try:
        model.eval()
        test_samples = dataset_list[:10]
        preds, refs = [], []
        
        # --- æœ¬åœ°ç®€æ˜“è®¡ç®— ROUGE-L (åŸºäºå­—ç¬¦çº§ LCS) ---
        def calculate_local_rouge(pred_str, ref_str):
            # å°†å­—ç¬¦ä¸²è½¬ä¸ºå­—ç¬¦åˆ—è¡¨ (å…¼å®¹ä¸­æ–‡å’Œè‹±æ–‡)
            x = list(pred_str.strip())
            y = list(ref_str.strip())
            if not x or not y: return 0.0
            
            # åŠ¨æ€è§„åˆ’è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ— (LCS)
            m, n = len(x), len(y)
            dp = [[0] * (n + 1) for _ in range(m + 1)]
            for i in range(1, m + 1):
                for j in range(1, n + 1):
                    if x[i - 1] == y[j - 1]:
                        dp[i][j] = dp[i - 1][j - 1] + 1
                    else:
                        dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
            lcs_len = dp[m][n]
            
            # è®¡ç®— F1 Score (ROUGE-L F1)
            # F1 = 2 * LCS / (len(pred) + len(ref))
            if (len(x) + len(y)) == 0: return 0.0
            return 2.0 * lcs_len / (len(x) + len(y))

        for item in test_samples:
            prompt = f"User: {item['instruction']}\n\nAssistant: "
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)
            pred = tokenizer.decode(outputs[0], skip_special_tokens=True).split("Assistant: ")[-1].strip()
            preds.append(pred)
            refs.append(item['output'])
        
        # è®¡ç®—å¹³å‡åˆ†
        scores = [calculate_local_rouge(p, r) for p, r in zip(preds, refs)]
        avg_score = sum(scores) / len(scores) if scores else 0.0
        print(f"ğŸ“Š {run_name} Manual-ROUGE-L: {avg_score:.4f}")
        
    except Exception as e:
        print(f"âš ï¸ Eval Error: {e}")

# ==========================================
# ä¸»æµç¨‹
# ==========================================
def main():
    print(f"ğŸŒŸ SFT Server Persistent Demo (Updated) å¯åŠ¨")
    
    # 1. å‡†å¤‡æœ¬åœ°æ¨¡å‹ (ç”± ModelScope é©±åŠ¨)
    model_path = get_local_model_path()
    
    # 2. å‡†å¤‡æœ¬åœ°æ•°æ®
    raw_data, dirty_indices_gt, oracle_data = prepare_data_local()
    
    # 3. è®¡ç®— & æ¸…æ´—
    sv = calculate_shapley(model_path, raw_data, oracle_data)
    
    n_remove = int(len(raw_data) * CONFIG['poison_ratio'])
    keep_indices = np.argsort(sv)[n_remove:]
    cleaned_data = [raw_data[i] for i in keep_indices]
    
    # Check
    removed_indices = np.argsort(sv)[:n_remove]
    recall = len(set(removed_indices).intersection(set(dirty_indices_gt))) / (len(dirty_indices_gt) + 1e-9)
    print(f"âœ… Recall: {recall:.2%}")

    # 4. è®­ç»ƒ
    run_sft_training(model_path, cleaned_data, "clean_model")
    
    print("\nğŸ‰ Done!")

if __name__ == "__main__":
    main()
