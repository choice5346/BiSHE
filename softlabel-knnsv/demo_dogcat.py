import sys
import os

# --- å¼ºåˆ¶è®¾ç½® PyTorch ç¼“å­˜è·¯å¾„åˆ° D ç›˜é¡¹ç›®ç›®å½• ---
# è¿™æ ·æ¨¡å‹ä¼šä¸‹è½½åˆ° D:\BiSHE\torch_cacheï¼Œä¸å†å ç”¨ C ç›˜
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
os.environ['TORCH_HOME'] = os.path.join(PROJECT_ROOT, 'torch_cache')
print(f"ğŸ”§ PyTorch æ¨¡å‹ç¼“å­˜è·¯å¾„å·²è®¾ç½®ä¸º: {os.environ['TORCH_HOME']}")

import numpy as np
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from sklearn.metrics import roc_auc_score, f1_score
from sklearn.cluster import KMeans
import time
import argparse
from tqdm import tqdm

# --- å¯¼å…¥åŒç›®å½•ä¸‹çš„æ ¸å¿ƒæ¨¡å— ---
try:
    from helper import knn_shapley_JW
except ImportError:
    # å¤‡ç”¨æ–¹æ¡ˆï¼šç¡®ä¿è„šæœ¬æ‰€åœ¨ç›®å½•åœ¨ path ä¸­
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    
    try:
        from helper import knn_shapley_JW
    except ImportError:
        print("âŒ é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ° helper.pyã€‚è¯·ç¡®ä¿è¯¥æ–‡ä»¶ä¸æœ¬è„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
        sys.exit(1)


class DualPoolingWrapper(nn.Module):
    """
    MaxSim æ€æƒ³å®ç°ï¼š
    åŒæ—¶ä¿ç•™ 'å…¨å±€ä¸Šä¸‹æ–‡ (Avg)' å’Œ 'æ˜¾è‘—æ€§ç‰¹å¾ (Max)'ã€‚
    è¾“å‡ºç»´åº¦ç¿»å€ã€‚
    """
    def __init__(self, feature_extractor):
        super().__init__()
        self.features = feature_extractor
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))
    
    def forward(self, x):
        # æå–ç‰¹å¾å›¾ (B, C, H, W)
        x = self.features(x)
        # å…¨å±€å¹³å‡æ± åŒ– (Context)
        x_avg = self.avg_pool(x).flatten(1)
        # å…¨å±€æœ€å¤§æ± åŒ– (Salience/MaxSim)
        x_max = self.max_pool(x).flatten(1)
        # æ‹¼æ¥ (B, 2*C)
        return torch.cat([x_avg, x_max], dim=1)

# ==========================================
# 0. ç‰¹å¾æå–å·¥å…· (Feature Extractor)
# ==========================================
def build_backbone(feature_type: str):
    """æ ¹æ®æŒ‡å®šåç§°æ„å»ºé¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼Œå¹¶å¯ç”¨ DualPoolingã€‚"""

    feature_type = feature_type.lower()
    base_model = None
    out_dim = 0

    if feature_type == 'resnet18':
        model = torchvision.models.resnet18(weights='IMAGENET1K_V1')
        # å‰¥ç¦»æœ€åçš„ avgpool å’Œ fcï¼Œåªä¿ç•™å·ç§¯éƒ¨åˆ†
        feature_extractor = nn.Sequential(*list(model.children())[:-2])
        model = DualPoolingWrapper(feature_extractor)
        out_dim = 512 * 2 # Concat
        
    elif feature_type == 'resnet50':
        model = torchvision.models.resnet50(weights='IMAGENET1K_V1')
        feature_extractor = nn.Sequential(*list(model.children())[:-2])
        model = DualPoolingWrapper(feature_extractor)
        out_dim = 2048 * 2

    elif feature_type == 'vgg11':
        model = torchvision.models.vgg11_bn(weights='IMAGENET1K_V1')
        # VGG çš„ features å°±æ˜¯å·ç§¯éƒ¨åˆ†
        feature_extractor = model.features
        model = DualPoolingWrapper(feature_extractor)
        out_dim = 512 * 2 # VGG11 æœ€åä¸€å±‚æ˜¯ 512é€šé“

    elif feature_type == 'mobilenet_v2':
        model = torchvision.models.mobilenet_v2(weights='IMAGENET1K_V1')
        # MobileNet çš„ features æ˜¯å·ç§¯éƒ¨åˆ†
        feature_extractor = model.features
        model = DualPoolingWrapper(feature_extractor)
        out_dim = 1280 * 2

    elif feature_type == 'densenet121':
        model = torchvision.models.densenet121(weights='IMAGENET1K_V1')
        feature_extractor = model.features
        model = DualPoolingWrapper(feature_extractor)
        out_dim = 1024 * 2 # DenseNet121 end dim
    
    # --- æ–°å¢å‰æ²¿æ¨¡å‹ ---
    elif feature_type == 'efficientnet_b0':
        # Google EfficientNet: æ•ˆç‡ä¹‹ç‹
        model = torchvision.models.efficientnet_b0(weights='IMAGENET1K_V1')
        feature_extractor = model.features
        model = DualPoolingWrapper(feature_extractor)
        out_dim = 1280 * 2
    
    elif feature_type == 'convnext_tiny':
        # Meta ConvNeXt: ç°ä»£ CNN çš„å·…å³°
        model = torchvision.models.convnext_tiny(weights='IMAGENET1K_V1')
        feature_extractor = model.features
        model = DualPoolingWrapper(feature_extractor)
        out_dim = 768 * 2
        
    elif feature_type == 'vit_b_16':
        # Vision Transformer: çº¯æ³¨æ„åŠ›æœºåˆ¶
        # ViT æ¯”è¾ƒç‰¹æ®Šï¼Œå®ƒçš„ç»“æ„ä¸é€‚åˆåš Spatial Pooling (å·²æœ‰ CLS token)
        # ä¸”æˆ‘ä»¬ä¹‹å‰çš„æµ‹è¯•æ˜¾ç¤ºå®ƒæ•ˆæœä¸€èˆ¬ï¼Œè¿™é‡Œæš‚ä¸åº”ç”¨ DualPooling
        model = torchvision.models.vit_b_16(weights='IMAGENET1K_V1')
        model.heads = nn.Identity() # è¾“å‡º 768 ç»´
        out_dim = 768
        print("âš ï¸ æ³¨æ„: MaxSim (DualPooling) æœªåº”ç”¨äº ViTï¼Œä»…ç”¨äº CNN æ¶æ„ã€‚")
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾ç±»å‹: {feature_type}")

    return model, out_dim


def extract_features(data_loader, feature_type: str):
    """ä½¿ç”¨æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹æå–æ·±åº¦ç‰¹å¾ã€‚"""

    print(f"ğŸ§  æ­£åœ¨ä½¿ç”¨ {feature_type} æå–æ·±åº¦ç‰¹å¾...")

    model, _ = build_backbone(feature_type)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    features_list = []

    with torch.no_grad():
        for inputs, _ in tqdm(data_loader, desc=f"Extraction-{feature_type}"):
            inputs = inputs.to(device)
            outputs = model(inputs)
            
            # --- å…³é”®ä¿®æ”¹ï¼šL2 å½’ä¸€åŒ– ---
            # è¿™ä¼šå°†ç‰¹å¾å‘é‡æŠ•å½±åˆ°å•ä½çƒé¢ä¸Š
            # ä½¿å¾—åç»­çš„æ¬§æ°è·ç¦»è®¡ç®—ç­‰ä»·äºä½™å¼¦è·ç¦» (1 - CosSim)
            outputs = torch.nn.functional.normalize(outputs, p=2, dim=1)
            
            features_list.append(outputs.cpu().numpy())

    return np.concatenate(features_list, axis=0)

# ==========================================
# 1. æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡ (Data Preparation)
# ==========================================
def get_dog_cat_data(n_train=200, n_val=100, flip_ratio=0.1, feature_type='raw'):
    print(f"ğŸ“¥ æ­£åœ¨å‡†å¤‡æ•°æ® (è®­ç»ƒé›†: {n_train}, éªŒè¯é›†: {n_val}, å™ªå£°ç‡: {flip_ratio}, ç‰¹å¾: {feature_type})...")
    
    # æœ¬åœ°æ•°æ®é›†è·¯å¾„é…ç½®
    LOCAL_DATA_ROOT = r'D:/newNLP/else/CATSVSDOGS/data/train_organized'
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(LOCAL_DATA_ROOT):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†è·¯å¾„: {LOCAL_DATA_ROOT}")
        return None

    # å›¾åƒé¢„å¤„ç†
    if feature_type == 'raw':
        # åŸå§‹æ¨¡å¼ï¼šç®€å•è°ƒæ•´å¤§å°ä»¥ç»Ÿä¸€å°ºå¯¸ (ä¾‹å¦‚ 64x64ï¼Œä¸ç„¶æ˜¾å­˜å¯èƒ½çˆ†ï¼Œè®¡ç®—ä¹Ÿæ…¢)
        transform = transforms.Compose([
            transforms.Resize((64, 64)), 
            transforms.ToTensor()
        ])
    else:
        # æ·±åº¦ç‰¹å¾æ¨¡å¼ï¼šç»Ÿä¸€åˆ° 224 å¹¶ä½¿ç”¨ ImageNet å½’ä¸€åŒ–
        transform = transforms.Compose([
            transforms.Resize((224, 224)), # å¼ºåˆ¶è°ƒæ•´å¤§å°
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    # åŠ è½½æœ¬åœ°æ•°æ®é›† (ImageFolderä¼šè‡ªåŠ¨æŠŠ cat/dog æ–‡ä»¶å¤¹è½¬ä¸º label 0/1)
    try:
        full_dataset = torchvision.datasets.ImageFolder(root=LOCAL_DATA_ROOT, transform=transform)
        print(f"ğŸ“‚ æˆåŠŸåŠ è½½æœ¬åœ°æ•°æ®é›†: {len(full_dataset)} å¼ å›¾ç‰‡")
        print(f"   ç±»åˆ«æ˜ å°„: {full_dataset.class_to_idx}") # ç¡®è®¤ä¸€ä¸‹ {'cat': 0, 'dog': 1}
    except Exception as e:
        print(f"âŒ æœ¬åœ°æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None
    
    # éšæœºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    np.random.seed(42)
    total_needed = n_train + n_val
    if total_needed > len(full_dataset):
        print(f"âš ï¸ è­¦å‘Šï¼šè¯·æ±‚çš„æ•°æ®é‡ ({total_needed}) è¶…è¿‡äº†æ€»æ•°æ®é‡ ({len(full_dataset)})ï¼Œå°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ã€‚")
        total_needed = len(full_dataset)
        n_train = int(total_needed * 0.8) # 80% è®­ç»ƒ
        n_val = total_needed - n_train
        
    all_indices = np.random.choice(len(full_dataset), total_needed, replace=False)
    train_idx = all_indices[:n_train]
    val_idx = all_indices[n_train:]

    train_subset = torch.utils.data.Subset(full_dataset, train_idx)
    val_subset = torch.utils.data.Subset(full_dataset, val_idx)

    # è·å–æ ‡ç­¾ (ImageFolder çš„ targets å±æ€§åŒ…å«äº†æ‰€æœ‰æ ‡ç­¾)
    def get_labels_from_full_dataset(full_ds, indices):
        return np.array([full_ds.targets[i] for i in indices])

    y_train = get_labels_from_full_dataset(full_dataset, train_idx)
    y_val = get_labels_from_full_dataset(full_dataset, val_idx)

    # --- æ ¸å¿ƒï¼šç‰¹å¾å‡†å¤‡ ---
    if feature_type != 'raw':
        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=False)
        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False)
        
        x_train = extract_features(train_loader, feature_type)
        x_val = extract_features(val_loader, feature_type)
    else:
        # åŸå§‹æ¨¡å¼ï¼šæ‰‹åŠ¨åˆ†æ‰¹è¯»å–é¿å…çˆ†å†…å­˜
        def get_flattened_data(subset):
            # å°†å¤§æ‰¹é‡æ‹†åˆ†è¯»å–
            loader = torch.utils.data.DataLoader(subset, batch_size=64, shuffle=False)
            data_list = []
            print(f"   æ­£åœ¨è¯»å–å¹¶å±•å¹³ {len(subset)} å¼ åŸå§‹å›¾ç‰‡...")
            for imgs, _ in tqdm(loader):
                # Flatten: (B, C, H, W) -> (B, -1)
                data_list.append(imgs.reshape(imgs.shape[0], -1).numpy())
            return np.concatenate(data_list, axis=0)
            
        x_train = get_flattened_data(train_subset)
        x_val = get_flattened_data(val_subset)

    # --- æ³¨å…¥å™ªå£° (Poisoning) ---
    n_flip = int(n_train * flip_ratio)
    dirty_indices = np.random.choice(n_train, n_flip, replace=False)

    print(f"ğŸ˜ˆ æ­£åœ¨æ³¨å…¥å™ªå£°ï¼šåè½¬ {n_flip} ä¸ªæ ·æœ¬çš„æ ‡ç­¾...")
    if len(dirty_indices) > 0:
        y_train[dirty_indices] = 1 - y_train[dirty_indices]

    return x_train, y_train, x_val, y_val, dirty_indices

# ==========================================
# 2. ä¸»æµç¨‹
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--feature_type',
        type=str,
        default='raw',
        choices=['raw', 'resnet18', 'resnet50', 'vgg11', 'mobilenet_v2', 'densenet121', 
                 'efficientnet_b0', 'convnext_tiny', 'vit_b_16'],
        help="é€‰æ‹©ä½¿ç”¨çš„ç‰¹å¾ç±»å‹"
    )
    parser.add_argument('--n_train', type=int, default=500, help="è®­ç»ƒæ•°æ®é‡")
    parser.add_argument('--n_val', type=int, default=100, help="éªŒè¯æ•°æ®é‡")
    args = parser.parse_args()

    print(f"ğŸš€ å¼€å§‹ Soft-label KNN-Shapley æ¼”ç¤º (Cat vs Dog) | æ¨¡å¼: {args.feature_type}")
    print("=" * 50)

    # 1. è·å–æ•°æ®
    data = get_dog_cat_data(
        n_train=args.n_train, 
        n_val=args.n_val, 
        flip_ratio=0.1, 
        feature_type=args.feature_type
    )
    
    if data is None: return
    x_train, y_train, x_val, y_val, dirty_indices = data

    print("\nğŸ” å¼€å§‹è®¡ç®—æ•°æ®ä»·å€¼ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
    start_time = time.time()
    
    # 2. è°ƒç”¨æ ¸å¿ƒç®—æ³• (K=5 æ˜¯å¸¸ç”¨å€¼)
    sv = knn_shapley_JW(x_train, y_train, x_val, y_val, K=5)
    
    duration = time.time() - start_time
    print(f"âœ… è®¡ç®—å®Œæˆï¼è€—æ—¶: {duration:.2f} ç§’")

    # 3. è¯„ä¼°æ•ˆæœ
    print("\nğŸ“Š è¯„ä¼°ç»“æœ (Evaluation):")
    print("-" * 50)

    # æ„é€  Ground Truth (1è¡¨ç¤ºè„æ•°æ®/æ ‡ç­¾åè½¬)
    true_labels = np.zeros(len(y_train))
    true_labels[dirty_indices] = 1

    # --- Metric 1: F1-Rank (åŸºäºTop-Kæ’åº) ---
    # æ¨¡æ‹Ÿ helper.py ä¸­çš„ kmeans_f1score(cluster=False)
    # é€»è¾‘ï¼šå–æœ€ä½åˆ†æ•°çš„ 10% ä½œä¸ºé¢„æµ‹çš„æœ€è„æ•°æ®
    threshold_rank = np.sort(sv)[int(0.1 * len(sv))]
    pred_rank = np.zeros(len(sv))
    pred_rank[sv < threshold_rank] = 1
    f1_rank = f1_score(true_labels, pred_rank)

    # --- Metric 2: F1-Cluster (åŸºäºKMeansèšç±») ---
    # æ¨¡æ‹Ÿ helper.py ä¸­çš„ kmeans_f1score(cluster=True)
    # é€»è¾‘ï¼šç”¨ KMeans æŠŠåˆ†æ•°èšæˆ2ç±»ï¼Œä¸­å¿ƒè¾ƒä½çš„é‚£ä¸€ç±»ä½œä¸ºè„æ•°æ®
    # æ³¨æ„ï¼šhelper.py çš„å®ç°æ˜¯ `val < min_center`ï¼Œæ¯”è¾ƒä¸¥æ ¼
    X = sv.reshape(-1, 1)
    kmeans = KMeans(n_clusters=2, random_state=0, n_init=10).fit(X)
    min_cluster_center = min(kmeans.cluster_centers_.flatten())
    pred_cluster = np.zeros(len(sv))
    pred_cluster[sv < min_cluster_center] = 1
    f1_cluster = f1_score(true_labels, pred_cluster)

    # --- Metric 3: AUROC ---
    # åˆ†æ•°è¶Šä½è¶Šå¯èƒ½æ˜¯è„æ•°æ®ï¼Œæ‰€ä»¥å–è´Ÿå·
    auc = roc_auc_score(true_labels, -sv) if len(dirty_indices) > 0 else 0

    print(f"Dataset Task: Mislabel Detection (Cat vs Dog)")
    print(f"Value Type  : {args.feature_type.upper()} + KNN-SV")
    print(f"Dirty Ratio : {len(dirty_indices)/len(y_train):.1%}")
    print("-" * 50)
    print(f"*** Evaluation Report ***")
    print(f"F1-Rank   : {f1_rank:.3f} (Top-10% cutoff)")
    print(f"F1-Cluster: {f1_cluster:.3f} (KMeans cutoff)")
    print(f"AUROC     : {auc:.3f}")
    print("-" * 50)
    
    if auc > 0.9:
        print("ğŸ‰ å®Œç¾ï¼(Excellent)")
    elif auc > 0.8:
        print("ğŸ‘ ä¸é”™ï¼(Good)")
    else:
        print("âš ï¸ æ•ˆæœä¸€èˆ¬ (Average)")

if __name__ == "__main__":
    main()
